{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from retriever_agent import Retriever\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import os\n",
    "import pprint\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up the logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_template():\n",
    "    \"\"\"\n",
    "    Define the prompt template for generating explanations based on the context and query.\n",
    "    \"\"\"\n",
    "    prompt_str = \"\"\"\n",
    "    You are an AI assistant specializing in explaining complex topics related to Retrieval-Augmented Generation(RAG).\n",
    "    Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query_str}\n",
    "\n",
    "    Please follow these guidelines in your response:\n",
    "    1. Start with a brief overview of the concept mentioned in the query.\n",
    "    2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "    3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "    4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "    Your explanation should be informative yet accessible, suitable for someone with a basic understanding of RAG.\n",
    "    If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer,\n",
    "    and only respond based on the given context.\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    prompt_tmpl = PromptTemplate(prompt_str)\n",
    "    return prompt_tmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_generation(state):\n",
    "    \"\"\"\n",
    "    Generate the prompt for the given search type, query, and reranking model.\n",
    "    \"\"\"\n",
    "    state = state\n",
    "    retriever_agent = Retriever(state)\n",
    "    reranked_documents = retriever_agent.retriever()\n",
    "\n",
    "    context = \"\\n\\n\".join(reranked_documents)\n",
    "    query = state.get('query')\n",
    "    prompt_templ = prompt_template().format(context_str=context, query_str=query)\n",
    "\n",
    "    return prompt_templ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGStringQueryEngine(CustomQueryEngine):\n",
    "    llm: OpenAI\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "\n",
    "    def custom_query(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for the given prompt using the LLM and response synthesizer.\n",
    "        \"\"\"\n",
    "        response = self.llm.complete(prompt)\n",
    "        summary = self.response_synthesizer.get_response(query_str=str(response), text_chunks=str(prompt))\n",
    "\n",
    "        return str(summary)\n",
    "    \n",
    "def create_query_engine(prompt: str):\n",
    "    \"\"\"\n",
    "    Create a query engine for generating responses based on the given prompt.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    response_synthesizer = TreeSummarize(llm=llm)\n",
    "\n",
    "    query_engine = RAGStringQueryEngine(\n",
    "        llm=llm,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    response = query_engine.query(prompt)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GenerationAgent(state: dict) -> OpenAIAgent:\n",
    "    \"\"\"\n",
    "    Define the GenerationAgent for generating explanations based on the user's query, search type, and reranking model.\n",
    "    \"\"\"\n",
    "\n",
    "    def generation(state):\n",
    "        \"\"\"\n",
    "        Generate an explanation based on the given search type, query, and reranking model.\n",
    "        \"\"\"\n",
    "        prompt = prompt_generation(state)\n",
    "        print(\"Passing the ReRanked documents to the LLM\")\n",
    "        response = create_query_engine(prompt)\n",
    "        print(\"Retrieved the summarized response from LLMs\")\n",
    "        #logger.info(\"Response:\")\n",
    "        #logger.info(response)\n",
    "        return response\n",
    "\n",
    "    def done(state):\n",
    "        \"\"\"\n",
    "        Signal that the retrieval process is complete, update the state, and return the response to the user.\n",
    "        \"\"\"\n",
    "        response = generation(state)\n",
    "        print(\"Retrieval process is complete and updating the state\")\n",
    "        state[\"current_speaker\"] = None\n",
    "        state[\"just_finished\"] = True\n",
    "        print(f\"Here is the answer to your query:{response}\")\n",
    "        return response\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=generation),\n",
    "        FunctionTool.from_defaults(fn=done),\n",
    "    ]\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant that is performing search and retrieval tasks for a retrieval-augmented generation (RAG) system.\n",
    "    Your task is to retrieve documents based on the user's query, search type, and reranking model.\n",
    "    To do this, you need to know the search type, query, and reranking model.\n",
    "    You can ask the user to supply these details.\n",
    "    If the user supplies the necessary information, then call the tool \"generation\" using the provided details to perform the search and retrieval process.\n",
    "    The current user state is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "    When you have completed the retrieval process, call the tool \"done\" with the response as an argument to signal that you are done and return the response to the user.\n",
    "    If the user asks to do anything other than retrieve documents, call the tool \"done\" with an empty string as an argument to signal that some other agent should help.\n",
    "    \"\"\"\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(state):\n",
    "        \"\"\"\n",
    "        Generate an explanation based on the given search type, query, and reranking model.\n",
    "        \"\"\"\n",
    "        prompt = prompt_generation(state)\n",
    "        print(\"Passing the ReRanked documents to the LLM\")\n",
    "        response = create_query_engine(prompt)\n",
    "        print(\"Retrieved the summarized response from LLMs\")\n",
    "        #logger.info(\"Response:\")\n",
    "        #logger.info(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a35009d49f4ccfb108975096545fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61c05a63afe4d08b6d6aa5c639ade35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0b580685674535a21b74d6dd699f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pavan\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--all_miniLM_L6_v2_with_attentions. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02869992f0464226a4536fe6b46538c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e6302536fc41eabb36066631073302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3127ffdc8c2a43d4af50d898855eea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stopwords.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d047d12b3185474a8f34c167e40c216e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab3fe57c49490fa29e00ef4721180a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/91.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n",
      "Passing the ReRanked documents to the LLM\n",
      "Retrieved the summarized response from LLMs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Self-RAG, or Self-Reflective Retrieval-Augmented Generation, is a framework that aims to enhance the quality and factuality of large language models by incorporating retrieval of relevant knowledge and self-reflection during the generation process. This approach addresses factual inaccuracies in LLM responses by allowing the model to adaptively retrieve passages on-demand and reflect on both the retrieved information and its own generated content using special reflection tokens. An example of Self-RAG in action could be seen in open-domain question answering, where the model dynamically retrieves relevant passages, generates a response, and reflects on the quality and factuality of the answer. Despite its potential, a limitation of Self-RAG is the complexity of training a single model to effectively perform retrieval, generation, and reflection tasks seamlessly. However, in the future, Self-RAG could find applications in fact verification, content generation, and complex reasoning tasks, enhancing AI systems' capabilities in handling diverse and challenging information sources.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = generation(state)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce0ddcdf351489982a434a67b2f7e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d600c73d6314a68ad046e8fb2bd11ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n",
      "Passing the ReRanked documents to the LLM\n",
      "Retrieved the summarized response from LLMs\n",
      "Retrieval process is complete and updating the state\n",
      "Here is the answer to your query:I have retrieved information on the Ragnarök framework using hybrid search with the crossencoder reranking model.\n"
     ]
    }
   ],
   "source": [
    "state = {   'chunk_overlap': None,\n",
    "'chunk_size': None,\n",
    "'current_speaker': None,\n",
    "'embedding_model': None,\n",
    "'input_dir': None,\n",
    "'just_finished': False,\n",
    "'query': 'what is self-RAG?',\n",
    "'reranking_model': None,\n",
    "'search_type': 'hybrid',\n",
    "}\n",
    "agent = GenerationAgent(state=state)\n",
    "response = agent.chat(\"I want to query what is a Ragnarök framework? Also can you use hybrid search along with crossencoder reranking model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
