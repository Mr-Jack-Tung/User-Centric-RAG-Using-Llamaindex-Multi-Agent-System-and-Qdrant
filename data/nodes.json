[
    {
        "id_": "19673776-ff32-4953-b184-1d1636bf9468",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2757fcfa-1e91-44c5-974a-6439c304677f",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "511c0fbcf700d501baec8bcc1d4ff5a6e43fbc65086108c34dbed3596b38f08b",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "d3a77a89-92b6-4432-9349-4564e2996b99",
                "node_type": "1",
                "metadata": {},
                "hash": "978890fa1e899c663f37bf9774c528613aa0f6c3eba4556d2eb05a1f24f1ece4",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove yuxiao qu1 tianjun zhang2 naman garg3and aviral kumar1 1carnegie mellon university2uc berkeley3multion acentralpieceinenablingintelligentagenticbehaviorinfoundationmodelsistomakethemcapableofintrospecting upon their behavior reasoning and correcting their mistakes as more computation or interaction is available even the strongest proprietary large language models llms do not quite exhibit the ability of continually improving their responses sequentially even in scenarios where they are explicitly told that they are making a mistake in this paper we develop riserecursiveintrospection an approach for finetuning llms to introduce this capability despite prior work hypothesizing that this capability may not be possible to attain our approach prescribes an iterative finetuning procedure which attempts to teach the model how to alter its response after having executed previouslyunsuccessfulattemptstosolveahardtesttimeproblemwithoptionallyadditionalenvironmentfeedback rise poses finetuning for a singleturn prompt as solving a multiturn markov decision process mdp where the initial state is the prompt inspired by principles in online imitation learning and reinforcement learning we propose strategies for multiturn data collection and training so as to imbue an llm with the capability to recursively detect and correct its previous mistakes in subsequent iterations our experiments show that rise enables llama2 llama3 and mistral models to improve themselves with more turns on math reasoning tasks outperforming several singleturn strategies given an equal amount of inferencetime computation we also find that rise scales well often attaining larger benefits with more capable models our analysis shows that rise makes meaningful improvements to responses to arrive at the correct solution for challenging prompts without disrupting oneturn abilities as a result of expressing more complex distributions 1 introduction a promising approach to utilizing and deploying foundation models and in particular large language models llms is to treat them as generalpurpose decisionmaking machines or agents to be successful an llm agent must not just provide plausible completions for input text but must exhibit interactive goaldirected behavior to accomplish a given task put in abstract terms this requires mastering two qualities aproducing responses that explicitly seek information about the task followed bybmaking decisions and improving them by thinking and verifying them at inference time for instance tosucceedinusinganewcodinglibrary aneffectivellmagentshouldfirstsynthesizeprograms then try the most promising subset against a compiler use the resulting feedback to improve the program and repeat the process for multiple turns having the ability to successfully improve a response in sequential attempts is equivalent to a form of selfimprovement at test time to enable testtime selfimprovement recent approaches attempt to repurpose the knowledge already stored in pretrained models via fewshot prompting  715315264 although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models it fails to produce models that can succeed in complex tasks by correcting their own mistakes such as those that require logical reasoning  2155 in many of these problems models contain the knowledge needed to answer a challenging prompt but are not able to elicit that knowledge even when asked to sequentially correct their mistakes finetuning the llm on domainspecific questionanswering data  62939 can help but it still does not teach the agent a testtime improvement strategy see section 6 a strategy for corresponding authors yuxiaoqandrewcmuedu this work was done at carnegie mellon universityarxiv240718219v2 cslg",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3849,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "d3a77a89-92b6-4432-9349-4564e2996b99",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2757fcfa-1e91-44c5-974a-6439c304677f",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "511c0fbcf700d501baec8bcc1d4ff5a6e43fbc65086108c34dbed3596b38f08b",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "19673776-ff32-4953-b184-1d1636bf9468",
                "node_type": "1",
                "metadata": {
                    "page_label": "1",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c97729cbbb20d70ae93ae08534b60d91541135c6f56af254031adfbdbbc4e8e2",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "still does not teach the agent a testtime improvement strategy see section 6 a strategy for corresponding authors yuxiaoqandrewcmuedu this work was done at carnegie mellon universityarxiv240718219v2 cslg 26 jul 2024",
        "mimetype": "text/plain",
        "start_char_idx": 3646,
        "end_char_idx": 3861,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3260f3b8-9e99-4ba8-9bfb-ca1ba8849b67",
        "embedding": null,
        "metadata": {
            "page_label": "2",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "fa252275-5504-4dee-bf13-f882dac7b014",
                "node_type": "4",
                "metadata": {
                    "page_label": "2",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "09a995bcef308fbbbdf5c734e3634bbcfdd5212c6b01f7ab10b856328661cd03",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove figure1recursive introspection rise using iterative multiround training on onpolicy rollouts and supervision from a reward function rise trains models that are capable of improving themselves over multiple turns at inference we run majority voting on candidate outputs from different turns to obtain the final response improving responses over sequential attempts at test time is crucial for tackling challenging prompts where directly attempting the problem in one shot may largely be futile can we train models to be capable of improving their ownresponses if done correctly and on a diverse set of problems and scenarios this could introduce in an llm a general procedure for howit can tackle a hard prompt by improving itself as opposed to supervising it with what to respond with which may not generalize as the test prompt becomes out of distribution although one straightforward approach to inducing this capability into a model would be to generate data that showcase improvements over multiple sequential turns potentially from highly capable models we find that simply imitating these data is not sufficient to enable this capability section 64 quite well this is due to two reasons first multiturn data from a different model would not show improvements in the kinds of errors the learner would make thereby being irrelevant to the learner  24 second often sequential multiturn data collected from proprietary models is also not of high quality since these models are typically not good at proposing meaningful improvements to their own errors  21 even though they can still provide useful responses to the problem at hand therefore we need a different strategy to endow models with a selfimprovement capability our key insight is to supervise improvements to the learners own responses in an iterative fashion taking inspiration from methods in online imitation learning  36 and reinforcement learning rl  45 this supervision can be in the form of oracle responses to the prompt sampled iid from more capable models or be generated from the learner itself our contribution is an algorithm rise recursive introspection figure 1 that utilizes these insights to improve the selfimprovement capability of an llm over the course of multiple attempts at a given prompt in each iteration our approach bootstraps onpolicy rollouts from the learner with better responses at the next turn obtained by running bestofn using a success indicator on the task on multiple revision candidates obtained by sampling from the learner itself or using responses from a more capable model whichever is more convenient in this way we are able to construct rollouts that demonstrate the learner how it can improve its responses under its own distribution then we finetune the learner on these data using a rewardweighted regression rwr  3435 objective that is able to learn from both high and lowquality parts of such rollouts by iteratively repeating this procedure we are able to instill a general selfimprovement capability into an llm our results show that llms trained via rise can produce correct responses on more prompts improving over turns for more challenging prompts even though strong base and intructiontuned llms  2358 often fail to improve their responses over multiplesequentialattemptsevenwhenexplicitlytoldabouttheirmistakespreviously risesuccessfully 2",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3432,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "4acef987-1f02-4129-9e2c-da9c81a6fb6d",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "cd5ad9ca-b708-4fc9-abea-275217876ffa",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "444c5ffb160034d2d3bb427507905f91bb24e9c9a7ff3a1aa42458c868201a9f",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "7427215a-7127-4d77-a565-dc470380b946",
                "node_type": "1",
                "metadata": {},
                "hash": "270a95a56e5fb4bf70e23a743c6bbb80971e87dcb31ad52f18db18480a21e052",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove endows similarlysized llms with selfimprovement capabilities resulting in monotonically increasing task performance after each turn specifically on the gsm8k  11 dataset rise demonstrates significant improvement over various models rise improves the performance of llama38b by 82 and mistral7b by 66 entirely using their own data rise attains a 177 improvement for llama27b over the course of 5turn introspection outperforming parallel sampling from the first turn and a 239 improvement for mistral7b in contrast gpt35 itself only improves by 46 over five turns we see similar trends on the math dataset  18 where rise improves llama27b by 46 and mistral7b by 111 over five turns we also study why and how rise is able to induce selfimprovement abilities and show that this ability generalizes to outofdistribution prompts as well these results consistently demonstrate rises effectiveness in enhancing mathematical reasoning capabilities for different models 2 related work several prior works build techniques to improve reasoning and thinking capabilities of foundation models for downstream applications typically these works focus on building prompting techniques for effective multiturn interaction with external tools  571432495456 sequentially refining predictions by reflecting on actions  71563 asking the model to verbalize its thoughts  335265 asking the model to critique and revise itself  3140 or by using other models to critique a primary models responses  2122054 although a subset of this work does improve its own responses this selfcorrection ability often requires access to detailed error traces eg execution traces from code compilers  731 in order to succeed in fact  21 and table 1 both indicate that selfimprovement guided by the llm itself ie intrinsic selfcorrection is often infeasible for offtheshelf llms even when they contain the knowledge required to tackle the prompt given but finetuning with rise induces this capability as we show in this paper beyond prompting previous work also attempts to finetune llm to obtain selfimprovement capabili ties 63962 these works attempt to improve reasoning performance by training on selfgenerated responses  3046575860 to achieve this these works use a combination of learned verifiers  28 4750 search  13263338 contrastive prompting on negative data  948 and iterated supervised or reinforcement learning rl  83759 although our approach also trains on modelgenerated data we aim to introduce a complementary capability to improve performance over sequential turns of interaction rather than to improve singleturn performance alone other work finetunes llms for multiturn interaction directly via rl 41 66 while this is indeed related singleturn problems posed in multiturn scenarios require addressing distinct challenges than generic multiturn rl isample efficiency is not a concern since the entire environment is fully characterized by the training dataset of prompts and oracle answers and dynamics are deterministic and iiwe need to generalize to novel test prompts multiturn rl focuses on sample efficiency which is not as critical in our setting though of course learning to generalize from a limited number of initial states would be appealing our main focus is to show that it is possible to train models for selfimprovement via appropriately designing multiturn finetuning objectives this is orthogonal from the choice of training approach rl or not the most related to our work are glore  17 and selfcorrect  53 which train separate models to identify errors and refine incorrect answers of other llms unlike these works",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3680,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7427215a-7127-4d77-a565-dc470380b946",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "cd5ad9ca-b708-4fc9-abea-275217876ffa",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "444c5ffb160034d2d3bb427507905f91bb24e9c9a7ff3a1aa42458c868201a9f",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "4acef987-1f02-4129-9e2c-da9c81a6fb6d",
                "node_type": "1",
                "metadata": {
                    "page_label": "3",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "bf81d462e20dd3dbb3a777ee852801aa3d1cf5a6ebfdd317ad205603ee4e92da",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "objectives this is orthogonal from the choice of training approach rl or not the most related to our work are glore  17 and selfcorrect  53 which train separate models to identify errors and refine incorrect answers of other llms unlike these works our approach trains a single model to produce answers and improve them over more than two turns which is the maximal number of turns studied in these works we show that doing so successfully requires careful design choices an iterative onpolicy data generation strategy along with a training objective that can learn from both successful and unsuccessful rollouts from an algorithmic point of view rise is similar to online 3",
        "mimetype": "text/plain",
        "start_char_idx": 3432,
        "end_char_idx": 4106,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c2a4f1d4-a6d8-4e8e-b89c-fe5068a05438",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "7911e640-6f3c-43c8-b127-55694584d47c",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "120129f860af05998a605b960fd8c2c5881dcf17b1968a58a7922284fab741c8",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "67ebe1a3-c3c5-4979-a21b-95221b256220",
                "node_type": "1",
                "metadata": {},
                "hash": "4a2afa5e6630aef4a0b735cd89fb7690aabc97e6e971f928b996e73e80ca8469",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove imitation learning  3644 in that it queries expert supervision on states attained by onpolicy rollouts onpolicy distillation for llms  14 utilizes this idea but queries an expert to provide completions on partial responses instead of sequential attempts that we do in this work 3 problem setup and preliminaries the goal of our work is to improve llm performance over sequential attempts  turns at a given problem concretely given a dataset \ud835\udc9f\ud835\udc65\ud835\udc56\ud835\udc66 \ud835\udc56\ud835\udc41 \ud835\udc561of problems \ud835\udc65\ud835\udc56and oracle responses \ud835\udc66 \ud835\udc56 our goal is to obtain an llm \ud835\udf0b\ud835\udf03\ud835\udc65\u02c6\ud835\udc661\ud835\udc61 \ud835\udc5d1\ud835\udc61that given the problem \ud835\udc65 previous model attempts \u02c6\ud835\udc661\ud835\udc61at the problem and auxiliary instructions \ud835\udc5d1\ud835\udc61eg instruction to find a mistake and improve the response or additional compiler feedback from the environment solves a given problem as correctly as possible to this end we encode this goal into the following learning objective that we wish to optimize max \ud835\udf0b\ud835\udf03\ud835\udc3f \ud835\udc561e\ud835\udc65\ud835\udc66\ud835\udc9f\ud835\udc66\ud835\udc56\ud835\udf0b\ud835\udf03\ud835\udc65\ud835\udc661\ud835\udc561\ud835\udc5d1\ud835\udc561i\u02c6\ud835\udc66\ud835\udc56\ud835\udc66 31 unlike standard supervised finetuning that trains the model \ud835\udf0bto produce a single response \u02c6\ud835\udc66given \ud835\udc65 equation 31 trains \ud835\udf0bto also appropriately react to a given history of responses from its own previous attempts \u02c6\ud835\udc661\ud835\udc561 equation 31 most closely resembles an rl objective and we will indeed develop our approach by converting a singleturn problem into a multiturn mdp finally note that promptingbased methods such as selfrefine  31 can still be viewed as training \ud835\udf0bto optimize \ud835\udf0b\ud835\udc66\ud835\udc65but only when only allowed to modulate the prompt \ud835\udc5d\ud835\udc56to optimize equation 31 naturally since the parameters \ud835\udf03are unchanged this would not be effective in optimizing the objective fully 4 rise recursive introspection for selfimprovement since even strong offtheshelf models do not exhibit an effective ability to improve themselves when provided with sequential attempts at a given problem  21 a natural next step is to ask how to train models to induce this capability in this section we will develop our approach rise for finetuning foundation models towards improving their own predictions over multiple turns our approach will first convert a problem into a multiturn mdp then collect data and finally run offline rewardweighted supervised learning in this multiturn mdp to induce this capability figure2left problem formulation we convert singleturn problems into multiturn mdps as discussed in section 41 the state is given by the prompt history of prior attempts and optional feedback from the environment an action is a response generated from the llm given the state of multiturn interaction so far right data collection we collect data by unrolling the current model \ud835\udc581times followed by an improved version of the response which is obtained by either 1 selfdistillation  sample multiple responses from the current model and use the best response or 2 distillation  obtain oracle responses by",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2865,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "67ebe1a3-c3c5-4979-a21b-95221b256220",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "7911e640-6f3c-43c8-b127-55694584d47c",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "120129f860af05998a605b960fd8c2c5881dcf17b1968a58a7922284fab741c8",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "c2a4f1d4-a6d8-4e8e-b89c-fe5068a05438",
                "node_type": "1",
                "metadata": {
                    "page_label": "4",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c4c7757a51f35718e0e91fdf3ee7ae347da2fb59f77e8d58617a9868ad3d62dd",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "the current model \ud835\udc581times followed by an improved version of the response which is obtained by either 1 selfdistillation  sample multiple responses from the current model and use the best response or 2 distillation  obtain oracle responses by querying a more capable model in either case rise then trains on the generated data 4",
        "mimetype": "text/plain",
        "start_char_idx": 2623,
        "end_char_idx": 2951,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7ab466f1-03e8-4386-90ff-c997dd987a76",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "47fcc1ab-f9cc-43ee-a86b-55c601d39463",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "557c14b982176a099426c4647ba6222a79e3ae23ba6527d8d23d3ff7efa9b5ab",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "18a79df5-0038-4137-beec-6568ade7e746",
                "node_type": "1",
                "metadata": {},
                "hash": "1fbdd36975bde970bb13e02c1db85ee2825fbce60876fae444eaf3f53e1aa9bb",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 41 converting singleturn problems into a multiturn markov decision process mdp the first step in building our approach is to procedurally construct a multiturn mdp out of a singleturn dataset of prompts and oracle responses figure 2 left given a dataset \ud835\udc9f\ud835\udc65\ud835\udc56\ud835\udc66 \ud835\udc56 consisting of prompts \ud835\udc65\ud835\udc56and corresponding oracle responses \ud835\udc66 \ud835\udc56eg math questions and natural language responses to those questions we will construct an induced mdp\u2133from\ud835\udc9f and then learn policies in this mdp an initial state in this mdp is a possible prompt \ud835\udc65\ud835\udc56\ud835\udc9f we denote the output response from the foundation model as action \ud835\udc4e given a state \ud835\udc60 the next state can be obtained by concatenating the tokens representing \ud835\udc60with the action \ud835\udc4eproposed by the model and an additional fixed prompt \ud835\udc53that asks the model to introspect eg this response is not correct please introspect and correct your answer the exact prompt is shown in appendix d4 the reward function is a sparse binary indicator of answer correctness at a given state \ud835\udc60\ud835\udc5f\ud835\udc65\ud835\udc56\ud835\udc4e  1if and only if \ud835\udc4e\ud835\udc66 \ud835\udc56and is obtained from an answer checking function this construction from dataset \ud835\udc9fto mdp\u2133is shown below \ud835\udc9f\ud835\udc65\ud835\udc56\ud835\udc66 \ud835\udc56  \u2133 \ud835\udf0c\ud835\udc600 unif\ud835\udc651\ud835\udc652\ud835\udc65\ud835\udc41 41 \ud835\udc43\ud835\udc60\ud835\udc60\ud835\udc4e \ud835\udeff \ud835\udc60concat \ud835\udc60\ud835\udc4e\ud835\udc53 42 \ud835\udc5f\ud835\udc60\ud835\udc4e 1\ud835\udc4e\ud835\udc66 \ud835\udc56if\ud835\udc65\ud835\udc56\ud835\udc60 43 42 learning in the multiturn mdp with the mdp construction in place the next step involves training a model to improve itself over the course of a rollout we subscribe to an offline approach to learning that we describe in the following step 1 data collection for selfimprovement to ensure that rollout data from this multiturn mdp is useful for teaching the model how to selfimprove it must satisfy a few desiderata 1it must illustrate the mistakes that the learner is likely to make and showcase how to improve upon them in the next attempt 2the data must illustrate responses that are relevant to the model given the problem and previous attempts in context and 3it must not contain any rollout that degrades in a subsequent turn our data collection strategy figure 2 right satisfies these desiderata in a given round \ud835\udc58 for a given problem \ud835\udc65\ud835\udc56 we unroll the currentmodel \ud835\udf0b\ud835\udf03\ud835\udc58to produce multiple sequential attempts denoted by \ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udf0b\ud835\udf03\ud835\udc58\ud835\udc60\ud835\udc56 \ud835\udc61 in problems where external input eg compiler feedback is available we also observe a variablelength natural language external input \ud835\udc53\ud835\udc56 \ud835\udc61eg in mathproblemsweaskthemodeltocorrectitself wealsoobserveascalarrewardvalue \ud835\udc5f\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61denoted",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2425,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "18a79df5-0038-4137-beec-6568ade7e746",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "47fcc1ab-f9cc-43ee-a86b-55c601d39463",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "557c14b982176a099426c4647ba6222a79e3ae23ba6527d8d23d3ff7efa9b5ab",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "7ab466f1-03e8-4386-90ff-c997dd987a76",
                "node_type": "1",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "4ce9a4bb5fd47913ffeb529811fc1d54ff346b8dd57128598af8a92df96a794d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "\ud835\udc61eg in mathproblemsweaskthemodeltocorrectitself wealsoobserveascalarrewardvalue \ud835\udc5f\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61denoted as\ud835\udc5f\ud835\udc56 \ud835\udc61in short let us denote this dataset of onpolicy model rollouts as \ud835\udc9fonpolicy \ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61 \ud835\udc53\ud835\udc56 \ud835\udc61 \ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc47 \ud835\udc611 for each timestep we construct an improved version of the response \ud835\udc66\ud835\udc56 \ud835\udc61that we will denote by \ud835\udc66\ud835\udc56 \ud835\udc61 we also record the reward score associated with this improved response as \ud835\udc5f\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61 or\ud835\udc5f\ud835\udc56 \ud835\udc61in short to obtain an improved version of a response \ud835\udc66\ud835\udc56 \ud835\udc61 we can employ several strategies perhaps the most straightforward approach is to query an offtheshelf more capable model to provide a correct response given the prompt \ud835\udc65\ud835\udc56 the previous response \ud835\udc66\ud835\udc56 \ud835\udc61 and an optional external feedback \ud835\udc53\ud835\udc56 \ud835\udc61 we refer to this as the distillation variant of our approach since it uses a strong teacher model to guide selfimprovement note that this is different from the classic notion of knowledge distillation and we will in fact show results in section 61 that will help understand the differences \ud835\udc9fonpolicy  distill  \ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61 \ud835\udc53\ud835\udc56 \ud835\udc61\ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc47 \ud835\udc611\ud835\udc9f \ud835\udc561 44 5",
        "mimetype": "text/plain",
        "start_char_idx": 2329,
        "end_char_idx": 3355,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "9aa47085-7111-428a-912b-0de73b87dbeb",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "68f32cd5-d181-465f-9644-dd39b22c108f",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "f9c99e36abbff255c4b3a4dd2031725b6adf0c11972ce0c17689022faf52c3cb",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "d83ecfe9-5d3f-473f-a0b4-c44d4a7ebca2",
                "node_type": "1",
                "metadata": {},
                "hash": "ee550a67eaf576c83ac4452a4a8ce91fd4dcedee3a0a5f1ac87f0020fbb90d10",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove the second variant of our approach which alleviates the need for a teacher model involves constructing an improved response by sampling multiple times from the learner itself we refer to this approach as the selfdistillation variant concretely for each state in the dataset \ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc9fonpolicy we sample \ud835\udc41responses \ud835\udc66\ud835\udc56 \ud835\udc610\ud835\udc66\ud835\udc56 \ud835\udc611\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc41\ud835\udf0b\ud835\udf03\ud835\udc60\ud835\udc56 \ud835\udc61 and use the best response from these \ud835\udc41candidates as measured by the associated reward values \ud835\udc5f\ud835\udc56 \ud835\udc610\ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc41 to relabel the model response at the next step \ud835\udc61 1in an improvement trajectory formally say \ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc5a  arg max \ud835\udc57\ud835\udc41\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc57 then we label the responses in the dataset\ud835\udc9fonpolicyat step \ud835\udc61 1with the improved response and its associated reward value \ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc5a \ud835\udc9fonpolicy  selfdistillation  \ud835\udc60\ud835\udc56 \ud835\udc611\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc5a \ud835\udc53\ud835\udc56 \ud835\udc611\ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc5a\ud835\udc471 \ud835\udc610\ud835\udc9f \ud835\udc561 45 step 2 policy improvement with the aforementioned data construction schemes we can now train a model on these datasets while in general any offline rl approach can be used to train on these data in our experiments we adopt an approach based on weighted supervised learning  35 due to ease of experimentation and its simplicity in particular we perform a weighted supervised regression where the weights are given by the exponential transformation of the reward values in \ud835\udc9f rewardweighted rl max \ud835\udf03e\ud835\udc65\ud835\udc56\ud835\udc9f\ud835\udc47 \ud835\udc611log\ud835\udf0b\ud835\udf03\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc60\ud835\udc56 \ud835\udc61exp\ud835\udc5f\ud835\udc61 \ud835\udc56\ud835\udf0f  46 where \ud835\udf0fis a temperature parameter to further expand or narrow the difference between good and bad actions in our preliminary experiments we found that equation 46 can often induce a bias towards increasing log likelihoods of responses where rewards are high prioritizing updates on easy problems where rewards are already high to address this issue we apply a slight modification to equation 46 and center the exponentiated rewards around the mean value averaged across all attempts on a given prompt akin to advantageweighted regression  34 we find that the use of advantages in place of rewards helps us avoid the richgetsricher phenomenon with easy problems 43 inference at deployment time rise can be run in two modes at inference time perhaps the most straightforward way to run the policy \ud835\udf0b\ud835\udf03trained by rise is within a multiturn rollout where the model samples a new response conditioned on the past context ie state in the multiturn mdp this past context consists of the external feedback \ud835\udc5dtest \ud835\udc56concerning the response \ud835\udc66test \ud835\udc56and the rollout terminates as soon as the",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2420,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "d83ecfe9-5d3f-473f-a0b4-c44d4a7ebca2",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "68f32cd5-d181-465f-9644-dd39b22c108f",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "f9c99e36abbff255c4b3a4dd2031725b6adf0c11972ce0c17689022faf52c3cb",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "9aa47085-7111-428a-912b-0de73b87dbeb",
                "node_type": "1",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "84ac21f06b0e146fba8c7014de3bb673a0ab9523d64c2d4c409491f28ca31ac9",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "new response conditioned on the past context ie state in the multiturn mdp this past context consists of the external feedback \ud835\udc5dtest \ud835\udc56concerning the response \ud835\udc66test \ud835\udc56and the rollout terminates as soon as the current response is judged to be correct according to the environments answer verification function put in other words we terminate the rollout as soon as the reward is equal to the reward for the oracle response \ud835\udc5f\ud835\udc65\ud835\udc66test \ud835\udc56 \ud835\udc5f\ud835\udc65\ud835\udc66 this protocol invokes queries to the reward function after each turn in the rollout since several reward function queries are performed we refer to this approach as with oracle  rise can also be run in a mode that avoids the need to query the answer checker or the reward function within a rollout in this case we run fulllength rollouts by forcing the model to retry ignoring the correctness of the response we then utilize a selfconsistency mechanism  51 based on majority voting to decide the candidate response at the end of each turn concretely at the end of each turn \ud835\udc57 we identify the response by running a majority vote over all response candidates from the previous turns maj \ud835\udc66test 1\ud835\udc66test 2\ud835\udc66test \ud835\udc57  including turn \ud835\udc57 we call this without oracle  a schematic illustration of these approach is shown in figure 3 most of our evaluations use no oracle 6",
        "mimetype": "text/plain",
        "start_char_idx": 2214,
        "end_char_idx": 3505,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "6b3de847-d7c5-46e4-b6d8-e5ac54bf9443",
        "embedding": null,
        "metadata": {
            "page_label": "7",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "8cf5efb6-9c40-43fa-a16b-72ef1a10e2ad",
                "node_type": "4",
                "metadata": {
                    "page_label": "7",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "d92240c57cf5b0a656b216a6ececbc9ffc4f9123af60cb3eb0d615e3f4d1865f",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove figure3rise inference there are two ways to query the model trained via rise upon inference 1 with oracle  left each time the model improves its response it is allowed to check its answer against an environment and terminate early as soon as a correct answer is found or 2 without oracle  right we ask the model to sequentially revise its own responses j times and perform majority voting on all candidate outputs from different turns to obtain the final response if the turn number \ud835\udc57is larger than the iteration number \ud835\udc58 the agent only keeps the most recent history with \ud835\udc58interactions to avoid testtime distribution shift at iteration \ud835\udc58 since the agent is able to improve its own response from \ud835\udc57to\ud835\udc57 1when \ud835\udc57\ud835\udc58 to avoid test time distribution shift in both modes we use a size \ud835\udc58shift window to store the most recent conversation history when the turn number \ud835\udc57is larger than the iteration number \ud835\udc58 44 practical algorithm and implementation details a complete algorithmic pseudocode for each approach is shown in appendix c we trained 7b models via rise and found that these models often could not adhere to response style and instructions for improving their responses when generating onpolicy data as a result before running onpolicy data collection we find it often useful to run an initial phase of supervised finetuning on indomain multiturn rollouts generated from a capable model to provide style and instructionfollowing information to the learner we call this the knowledge boosting stage we then run onpolicy rollouts starting from a boosted model in each iteration we generate 1 trajectory for each unique problem we then run finetuning with hyperparameters and details in appendix d for iterative finetuning we find that starting from the basemodel but training on data from all iterations thus far is more beneficial than continued finetuning from the checkpoint obtained in the previous iteration 5 when and why is selfimprovement over turns possible a natural question to ask is why selfimprovement with riseeven possible one might surmise that the model may simply not have enough knowledge to correct its ownmistakes if it is unable to correctly answer the problem in the first turn then why is it possible to teach the model to correct its own mistakes in this section we provide the reason why this kind of selfimprovement is possible supported with empirical evidence to justify our hypotheses iteratively teaching a model how to make updates on a given response can be crucial when representing the target distribution \ud835\udc5d\ud835\udc66\ud835\udc65requires more capacity than what the model \ud835\udf0b\ud835\udf03affords by conditioning on only the input prompt tokens when the target distribution requires greater capacity learning a sequence of conditionals \ud835\udf0b\ud835\udf03\ud835\udc66\ud835\udc561\ud835\udc65\ud835\udc660\ud835\udc56followed by marginalization is expected to induce a more 7",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2870,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f40ab9fd-5fe2-4f61-a502-ea6f09d2214c",
        "embedding": null,
        "metadata": {
            "page_label": "8",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9460259b-d597-42d0-8f09-964e62a0b152",
                "node_type": "4",
                "metadata": {
                    "page_label": "8",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "38c91876a7e591d3ba4076f782a858b5fd3f23286f38577b33fd77cfc724e5a0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove figure4the probability of the true answer given the prompt observe that model trained with rise has higher probability for the true answer figure5the training perplexity loss of fitting only the oracle answer or a sequence of answers observe that fitting a sequence of answers rise reduces the loss more than fitting only the oracle answer classic flexible marginal distribution over \ud835\udc66\ud835\udc47given \ud835\udc65 this hypothesis is akin to the difference between diffusion models  42 and variational autoencoders vaes  25 in image generation iteratively fitting a sequence of generative distributions over intermediate noisy inputs in a diffusion model gives rise to a more flexible distribution  43 than monolithic variational autoencoding even though diffusion models still utilize an evidence lowerbound objectiveelbo while the diffusion process utilizes handdesigned noise schedules rise utilizes the base model itself to induce iterative improvements to verify if this hypothesis is true we tracked the training unweighted negative loglikelihood loss nll values for the oracle response \ud835\udc66given the input prompt \ud835\udc65marginalized over intermediate steps in a multiturn rollout and compared it against the nll values log\ud835\udc5d\ud835\udf03\ud835\udc66\ud835\udc65attained by directly attempting to predict the final response in figure 4 labeled as classic concretely we sampled 256 prompts \ud835\udc65 and their oracle responses \ud835\udc66and computed the average log\ud835\udc5d\ud835\udf03\ud835\udc66\ud835\udc65across all \ud835\udc65 along with a 95 confidence interval for different checkpoints during training we find that for any given number of epochs including fractional number of epochs on the xaxis the nll value is lower when conditioning on multiturn data that rise generates in comparison with oracle responses to the prompts obtained from an expert this suggests that rise is able to utilize the computation of tokens from previous turns to model the target distribution we also measure the average nll loss on all samples through training sampled iid from the training dataset for rise and classic finetuning and observe a similar trend rise is able to reduce loss more than the standard approach attaining lower perplexity values figure 5 of course in problems that require knowledgebased question answering it is not possible for the model to produce any meaningful improvements because learning \ud835\udc5d\ud835\udc66\ud835\udc65is not bounded by insufficient capacity of \ud835\udf0b\ud835\udf03\ud835\udc66\ud835\udc65 but is rather unable to match \ud835\udc5ddue to the absence of features that are critical to learn the correct mapping from \ud835\udc65to\ud835\udc66 we expect that training with rise would only incentivize hallucinations in this case  24 since more input tokens appearing from previous attempts would only provide easier ways to pick up on spurious correlations however this is not the failure mode on reasoning problems  27 where majk rates at turn 1 tend to be higher than pass1 as we find in our experiments indicating that performance can be improved by sampling the model itself in fact in figure 6 we also show that the sequential procedure learned by rise can even solve a significant fraction of problems that were unsolved by passb for much larger \ud835\udc35in the first turn indicating that it learns to index into the 8",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3196,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7395e4e5-c68c-41c3-8bd7-0fced62379f0",
        "embedding": null,
        "metadata": {
            "page_label": "9",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "8af10def-a233-4b59-bc18-20b42c02f5da",
                "node_type": "4",
                "metadata": {
                    "page_label": "9",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "e30fdfe053d1b4cfd4b08119bb5b2fd29fe49c308d60c325a907d92e2d83d9d1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove pretrained knowledge of the model in a different manner as opposed to simply translating the passk performance into the pass1 performance of the model that majority of singleturn approaches are believed to be doing figure6fraction of problems unsolved by passb at the first turn that sequential 5turn sampling from rise solves where \ud835\udc35 5\ud835\udc58\ud835\udc58is the xaxis rise can solve several challenging problems that sampling at the first turn with much larger budgets cannot solve 6 experimental evaluation the goal of our experiments is to demonstrate the efficacy of rise in instilling language models with the ability to selfimprove their responses over turns our experiments answer the following questions 1 how effectively can rise improve performance over multiple sequential attempts ie turns at a given prompt 2does the performance of rise improve with more rounds of iterative training 3does the selfimprovement strategy induced by rise generalize to novel problems that are out of the training domain and finally 4what is the best data composition for training rise to this end we compare rise to other prior and baseline approaches and perform ablations on gsm8k 11 and math 18 baselines comparisons and evaluation we compare rise to several prior methods that attempt to induce similar selfimprovement capabilities a selfrefine 2131 that prompts a base model to critique and revise its mistakes b glore 17 which trains a separate reward model to locate errors and a refinement model to improve responses of a base llm and c selfconsistency 51 which runs majority voting on multiple responses from the first turn as a baseline to compare to our sequential strategy we tried to construct fair comparisons between rise and these methods by using a similarsized model  2358 but differences in the base model training data and evaluation setups still prohibits us from performing an applestoapples comparison in some cases nonetheless we can still hope to understand the ballpark of improvement by contextualizing our results with these prior works we also compare to vstar 19 but since this is not an fair comparison we defer it to appendix b we evaluate rise in both modes at inference time with and without an oracle section 43 at the end of five turns concretely these metrics are defined as follows with oracle p1t5  this run terminates the rollout as soon as the response is correct in other words this metric allows queries to the final answer verifier at the end of each turn withoutoraclem1t5  thisrundoesnotterminatetherolloutbeforefiveturnsandwecompute the maj1 performance on the candidates produced in each turn as detailed in section 43 we also compare majk performance at the first turn for all the models we train m1t1 m5t1 9",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2804,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "d39bbb85-ba58-4308-a553-def37e7c0890",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "528a8650-aabc-43e5-8953-0eb3c9da3a02",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c8db8dab2bd3899b5e7979ab96be27af8b7151c620aceb78adffac077c81c9b7",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "b44c4362-3fef-4312-bb45-f6d6e4a144a3",
                "node_type": "1",
                "metadata": {},
                "hash": "2a90df273e3bb1fa026af8b586eddf9fe083b321f8724b305d0334ee5a965ce7",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove approachgsm8k 10 math18 wo oracle w oracle wo oracle w oracle m1t1m5t1m1t5 p1t5 m1t1m5t1m1t5 p1t5 rise ours llama2 base 105 228 123 11106 13934 19 51 32 1405 2304 boost 329 454 125 39263 555226 55 68 13 5500 14691 iteration 1 356 497 141 507151 639283 63 88 25 9734 194131 iteration 2 373 510 137 550177 684311 58 104 46 10446 198140 sft on oracle data only correct data 274 422 149 34066 436162 58 79 21 5503 12162 correct and incorrect 257 418 161 31255 415158 50 52 02 5000 13181 rise ours mistral7b 337 494 157 39053 469132 75 130 55 8409 13055  iteration 1 353 506 153 59223968633367 95 28 184111297224 7b sota 58 eurus7bsft 363 663 300479116 531168 123 198 7516340 229106 selfrefine 31 m1t3p1t3 m1t3p1t3 base 105 224 119 7134 13025 19 51 32 1900 3112 iteration 2 373 505 132 33340 44572 58 94 36 5701 9537 gpt35 664 802 138 61054 71652 397 465 68 36532 46770 mistral7b 337 485 148 212125 37942 75 123 48 7104 11439 eurus7bsft 363 659 296 262101 42865 123 194 71 9033 15128 glore 17 m1t3p1t3 orm 482 495 13 57189 sorm 482 516 34 597115 not studied in 17 direct 482 474 08 592110 table 1 rise vs other approaches selfrefine glore and baselines observe that rise attains the biggest performance improvement in brown between 1turn m5t1 and 5turn m1t5 performance wo an oracle on both gsm8k and math this performance gap is even larger when oracle early termination is allowed p1t5 w oracle selfrefine  31 degrades performance across the board when used without an oracle and attains minor performance improvements when used with an oracle glore trains a separate refinement model but still performs worse than rise more details about it are in appendix b using rise on top of a better base model mistral7b is also effective positive improvements with multiple turns and note the m1t5 performance of mistral7b exceeds even stateoftheart math models such as eurus7bsft  58 simply running singleturn sft on data utilized by rise",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1999,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "b44c4362-3fef-4312-bb45-f6d6e4a144a3",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "528a8650-aabc-43e5-8953-0eb3c9da3a02",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c8db8dab2bd3899b5e7979ab96be27af8b7151c620aceb78adffac077c81c9b7",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "d39bbb85-ba58-4308-a553-def37e7c0890",
                "node_type": "1",
                "metadata": {
                    "page_label": "10",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "82ae1fc13cd250fd627a6c6bf6a94219295f42c03c8a089375a717e3f6fc4715",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "is also effective positive improvements with multiple turns and note the m1t5 performance of mistral7b exceeds even stateoftheart math models such as eurus7bsft  58 simply running singleturn sft on data utilized by rise is not effective at inducing a selfimprovement capability implying that the algorithmic design choices in rise are crucial for performance color coding indicates numbers that can be compared to each other 61 does rise improve performance over multiple turns compared to other approaches main results we present the comparisons in table 1 first note that rise iteration 1 and iteration 2 boosts up the llama2 base models fiveturn performance by 151 and 177 respectively with each iteration on gsm8k and 34 and 46 on math wo any oracle interestingly we found using promptingonly selfrefine  31 largely degrades performance across the board even with a strong proprietary model gpt35 the strongest 7b base models mistral7b and eurus7bsft  58 when coupled with standard prompting are only able to improve their performance but only by 53  116 and 09  40 respectively on gsm8k and math which is significantly lower than our approach the performance of glore improves only by 34 on gsm8k over two turns but this is still lower than our approach which improves by 63 in two turns and 134 in three turns see appendix b1 this indicates that rise is effective in teaching models how to improve their own errors to summarize training with rise gives the largest performance improvement gains compared to other approaches both with and without the use of an oracle and these gains are transferred to other base models one might also hypothesize that the performance gains with rise here are largely a result of utilizing 10",
        "mimetype": "text/plain",
        "start_char_idx": 1780,
        "end_char_idx": 3511,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f95f4aad-b3e8-4080-8c9a-1692aa8452e5",
        "embedding": null,
        "metadata": {
            "page_label": "11",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "183ca8ec-1e21-42d6-a027-7d6f072979ae",
                "node_type": "4",
                "metadata": {
                    "page_label": "11",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "80c617ffa34c3a389ecb4f7ccf7788829cb7a5d315bf89f9543e1128076b864e",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "f1b37453-7e2e-45a2-a052-98f3a3b9fdbd",
                "node_type": "1",
                "metadata": {},
                "hash": "923389522825f5ce3d5513f4818dbfbbe0f43aada7964cd81b986671fe7160b5",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove queries to an offtheshelf more capable model for providing supervision and not the algorithmic approach for data collection and training to address this hypothesis we store all the data generated by rise from more capable models and train on this data via standard singleturn sft  sftonoracledata  since not all of this data are guaranteed to be correct we also run this experiment on only the correct responses in these oracle data observe in table 1 that this procedure does not still instill selfimprovement capabilities largely preserving or degrading sequential  maj1turn5  performance compared to simply sampling one response in the first turn this means that the algorithmic design of rise is critical in enabling it to learn selfimprovement capabilities as opposed to simply the use of expert supervision 611 can rise effectively make use of mistakes and correct them one concern that arises from prior results on selfrefinement or selfcorrection is whether the model can truly correct itself over turns or whether the improvement comes from the effect of sampling more answers and picking the best one in table 1 we see that sequentially improving responses via rise maj1turn5  outperforms sampling 5 responses in parallel at the first turn and applying a majority vote on them  maj5turn1  please note that this comparison utilizes an equal number of samples with the only difference being that these samples are drawn in parallel at the first turn in one case and sequentially at the end of five turns in the other comparing maj5 performance at the end of 1 turn and 5 turns we observe a consistent 4 to 8 improvement on gsm8k and an 65 improvement on math with mistral7b model this means that rise can imbue models with a selfimprovement ability while running parallel sampling alone on any model cannot endow the same ability even the maj5turn1 performance of standard singleturn sft on the data used by rise is substantially worse than the sequential maj1turn5 performance of rise implying that the algorithmic protocol of rise plays a critical underlying role finally we also remark that in figure 6 we showed that the sequential procedure learned by rise over five turns could solve a significant fraction of problems that were unsolved by passb for much larger values of \ud835\udc355in the first turn implying that sequential rise can actually tackle prompts that were not solvable by simply sampling more responses in the first turn one might also speculate if these improvements in sequential improvement ability largely come at a cost of reduced improvements in first turn performance in addition we also observe that running multiple iterations of rise still preserves the first turn performance while improving the 5turn performance 612 how does the base model affect rise the performance of rise with llama27b on an absolute scale is lower than the best models specifically finetuned on math data eg eurus7bsft or mistral7b however we find that rise is still effective on top of mistral7b base model in fact our performance at the end of five turns outperforms one of the best 7b sft models customized to math reasoning  compare the m1t5 performance of eurus7bsft and mistral7b in rise ours to find that mistral7b  rise outperforms eurus7bsft 613 selfdistillation version of rise we also compare the performance of rise with entirely selfgenerated data and supervision equation 44 \ud835\udc41 16 after one iteration directly on top of more capable models",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3528,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f1b37453-7e2e-45a2-a052-98f3a3b9fdbd",
        "embedding": null,
        "metadata": {
            "page_label": "11",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "183ca8ec-1e21-42d6-a027-7d6f072979ae",
                "node_type": "4",
                "metadata": {
                    "page_label": "11",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "80c617ffa34c3a389ecb4f7ccf7788829cb7a5d315bf89f9543e1128076b864e",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "f95f4aad-b3e8-4080-8c9a-1692aa8452e5",
                "node_type": "1",
                "metadata": {
                    "page_label": "11",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c463dc665eee1152665fa95f401cff1cab0c3da08a10715b1a1c1dfd752eaf03",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "rise outperforms eurus7bsft 613 selfdistillation version of rise we also compare the performance of rise with entirely selfgenerated data and supervision equation 44 \ud835\udc41 16 after one iteration directly on top of more capable models mistral7b and llama38b on gsm8k in table 2 without any knowledge boosting phase we find that this variant also improves the 5turn performance of the base model compared to the first turn compare m1t5 vs m1t1 for both the models llama38b and mistral7b where rise boosts the sequential selfimprovement performance by more than 1 compared to turn 1 performance wo any oracle of course we also note that this version of rise does not outperform the m5t1 performance of the 11",
        "mimetype": "text/plain",
        "start_char_idx": 3299,
        "end_char_idx": 4000,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7facae39-128d-4dc0-a7a6-ec1ac7a69da4",
        "embedding": null,
        "metadata": {
            "page_label": "12",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e3b7d6c7-5380-4f9f-80d8-838dc880a175",
                "node_type": "4",
                "metadata": {
                    "page_label": "12",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "6360302b8e5710fd2f01e6dda7c91b8a7f546f6bef9ae7ce28d0c8539c2a9c45",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "7735bfd5-ef2d-49f3-8e00-ce628859ce46",
                "node_type": "1",
                "metadata": {},
                "hash": "13160c7a9c50007a2e3c6a7b82b91afd62385110f85630593df725d0f7080a65",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove rise selfwo oracle w oracle m1t1m5t1m1t5 p1t5 mistral7b 337 494 157 39053 469132  iteration 1 368 444 76 39566 487159 llama38b 453 697 244 52572 610157  iteration 1 656 807 151 73882 812156 table 2 rise with selfdistillation on gsm8k rise is able to improve 5turn maj1 performance of the model with entirely selfgenerated data and supervision despite the fact that the base mistral7b model does not produce correct answers for several problems finetuned model we expect this to be largely a function of one single iteration of training since the selfdistillation version of rise utilizes bestofn sampling against the same model to produce supervision for selfimprovement rise would first have to match the performance of bestofn sampling before it can start to improve over it via reward maximization due to the significant gap between the base models m5t1 and m1t5 performance we expect that this will take quite a few iterations or a fully online rl algorithm we did not have computational resources and infrastructure to run multiple iterations but this is an interesting avenue for future work in this selfdistillation setting we could also divide the computation between sequential and parallel sampling strategies to get the best results at the end of five turns nonetheless this result shows that even by training on selfgenerated samples rise can actually amplify the sequential sampling performance of the base model 62 does the performance of rise improve with iterative training next we attempt to understand if rise improves with multiple rounds of training on onpolicy data as shown in tables 1 and 2 the performance of rise improves from iteration to iteration constantly the 5turn performance of rise both with and without an oracle exhibits a clear improvement with more rounds this implies that iterative selftraining procedures of the form of star  61 can also be combined with rise to train models for selfimprovement this also perhaps serves as a strong hint towards the potential utility of full online reinforcement learning rl techniques 63 does rise also improve sequential performance on outofdistribution prompts in table 3 our aim is to evaluate the robustness of the strategy induced by rise on new unseen prompts specifically we compare the performance of the rise model trained with a dataset on evaluation prompts from another dataset note in table 3 these datasets include math gsm8k and svamp generally we observe that the model trained on one dataset is still able to improve the base models performance on another dataset over the course of sequential five turns more concretely while the base llama2 model largely degrades its turn 1 performance over turn 5 performance models trained with rise enable a positive performance improvement on these outofdistribution prompts this means that even though these models have not seen queries similar to the evaluation dataset simply training with rise onsomekind of mathematical prompts still boosts the efficacy of the selfimprovement strategy on a new distribution of test prompts this finding suggests that rise is capable of instilling selfimprovement procedures that can generalize beyond the distribution of prompts in the finetuning data 64 what data compositions and data quantity are crucial for rise we now study how different data compositions affect the performance of rise with the goal of answering questions such as should we collect onpolicy error correction data like dagger  36 or should we bias towards highquality offpolicy",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3594,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7735bfd5-ef2d-49f3-8e00-ce628859ce46",
        "embedding": null,
        "metadata": {
            "page_label": "12",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e3b7d6c7-5380-4f9f-80d8-838dc880a175",
                "node_type": "4",
                "metadata": {
                    "page_label": "12",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "6360302b8e5710fd2f01e6dda7c91b8a7f546f6bef9ae7ce28d0c8539c2a9c45",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "7facae39-128d-4dc0-a7a6-ec1ac7a69da4",
                "node_type": "1",
                "metadata": {
                    "page_label": "12",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "6fc4c3a1cbd83e2ac5f51d654e4a257a65989cd8e9155d9b29c46ecd3f8920c5",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "compositions and data quantity are crucial for rise we now study how different data compositions affect the performance of rise with the goal of answering questions such as should we collect onpolicy error correction data like dagger  36 or should we bias towards highquality offpolicy data  to understand the utility of different data compositions we enlist 12",
        "mimetype": "text/plain",
        "start_char_idx": 3309,
        "end_char_idx": 3670,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f40bb11f-3c2c-4dcb-a4be-ed1f0eaefa83",
        "embedding": null,
        "metadata": {
            "page_label": "13",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9350d9d4-5889-49d1-b181-7eb09c3043cc",
                "node_type": "4",
                "metadata": {
                    "page_label": "13",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "49cf5c57105c7bda676af1d5b2fd5d336730c3699dc95cbfa4228ba6b9764b4d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove risewo oracle w oracle m1t1m1t5 p1t5 gsm8k llama2 base 105 111 06 13934 iteration 1 rise model trained on math 193 326 133 484291 math llama2 base 19 14 05 2304 iteration 1 rise model trained on gsm8k 43 44 01 12178 svamp llama2 base 292 305 13 34048 iteration 1 rise model trained on math 301 314 12 459158 iteration 1 rise model trained on gsm8k 422 500 78 636214 table 3 outofdistribution generalization of rise we evaluate model finetuned on math on the gsm8k test set model finetuned gsm8k on math and the model finetuned on a mixture of gsm8k and math on the svamp data observe even though we train on ood prompts rise can still improve sequential performance the three aspects rise athe use of multiturn rollout data for finetuning bthe use of unsuccessful  suboptimal rollouts via weighted supervised finetuning compared to na\u00efve supervised learning which only utilizes successful rollouts for finetuning and cthe use of onpolicy rollouts and selfgenerated or oracle data we will now perform controlled experiments to understand the effect of each of these factors on the overall performance of rise figure7left the importance of multiturn interaction history and weighted objectives for training rise note that training with multiturn data leads to better selfimprovement performance at the end of 5 turns than oneturn data obtained from the original dataset with oracle answers from another model also observe that using a weighted objective performs better right the importance of using all rollouts for learning  instead of only successful rollouts or only successful responses in the data using all data performs best in our results a data composition for finetuning we first study the necessity of using the interaction of error correction history for training rise in figure 7 left we compare two approaches model trained with oracle answers shown right after the query 1turn and oracle answers shown after intermediate failed attempts multiturn in figure 7 left even though the latter trains on intermediate responses that may not always be correct it attains a higher performance than simply training on the correct response for a given prompt this highlights the importance of training on contexts that include a multiturn interaction history depicting mistakes from the learner to improve selfimprovement capabilities b weighted supervised learning vs unweighted supervised learning next we investigate the effect 13",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2507,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "8f17c84d-3d27-42fd-81c3-515380a9a316",
        "embedding": null,
        "metadata": {
            "page_label": "14",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "b4dcfa88-ddec-44f7-981e-cee628f51fdc",
                "node_type": "4",
                "metadata": {
                    "page_label": "14",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "fa504f159af0e0eb7b56314df95972407379cffd1ab125059b0c15529a64a0e1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove of rewardweighted rl on multiturn data in rise as opposed to simply imitating filtered successful data we find that using all the data leads to improved performance over simply filtering good datain figure 7 right which reduces sample size in figure 7 left we find that rewardweighted training improves performance on later turns allowing us to better leverage all the suboptimal data c onpolicy vs offpolicy data selfgenerated vs expert data rise runs onpolicy rollouts and seeks improvements on responses that the learner produces as shown in figure 8 left a dagger  36style approach that seeks improvements on responses appearing in onpolicy rollouts improves performance greenorange compared to using expert data alone bluepink conceptually this addresses the traintest mismatch between the distribution of context tokens enabling imitation learning methods to now target the correct distribution in addition recent work  24 has shown that llms often memorize unfamiliar examples generated by oracle models by training on onpolicy rollouts we should be able to eliminate any such potential issues thus while the model trained via offline imitation is able to reduce loss these improvements do not generalize to new problems figure8left the importance of data sources used for training we study the performance of the iteration 1 of rise on gsm8k with different data sources expert refers to the use of an oracle model onpolicy corresponds to sampling from the learner and bestofn means using the best sample out of \ud835\udc41from the learner here \ud835\udc41 16right comparing rise with oracle error feedback pass1  turn k solid lines to parallel sampling of 5 responses at turn 1 passk  turn 1 dashed lines over number of turns \ud835\udc58on the xaxis on gsm8k observe that sequential sampling with iteration 1 and iteration 2 rise models consistently outperforms parallel sampling for all values of turn \ud835\udc58 and the gap grows as the number of iterations increases in contrast this trend is absent for base and sft models 65 passk vs sequential sampling via rise we now study the performance of sequential sampling with oracle feedback in gsm8k unlike relying on majority voting as in table 1 specifically we compare the performance of rise with early termination of evaluation rollouts against pass5 not maj5 performance of the rise model at the first turn which makes an equal number of queries to the groundtruth correctness indicator access to groundtruth correctness indicator is expected to improve performance for both parallel and sequential sampling unsurprisingly but we see in figure 8 right that rise is able to improve performance more beyond simply sampling more samples at the first turn and computing passk despite this strong assumption of access to an oracle final answer verifier made by the parallel sampling approach we would expect parallel sampling via passk to be most performant when provided access to oracle answer checking as the model can choose to simply sample \ud835\udc3eindependent responses if the base model accuracy on this task is reasonable passk  turn 1 also upper bounds the first turn accuracy of any procedure that does not query the oracle eg with verifiers with majority voting etc hence access 14",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3277,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "a22bec5b-b5ae-4b1c-aca7-db354393e060",
        "embedding": null,
        "metadata": {
            "page_label": "15",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "eddb63ca-5726-43c3-b4c4-52b399be9a40",
                "node_type": "4",
                "metadata": {
                    "page_label": "15",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "0a3316fa3d5fe0231509daa6bf00ab6d83ade5efc3259db7e69a1a99fe73bc97",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove to oracle answer checking for each individual response presents the strongest result one couldexpect out of parallel sampling in one turn on the other hand sequential sampling produces correlated samples and hence should in principle not be able to improve over parallel sampling unlessthe model is unable to use the additional tokens and computation provided by the feedback selfimprovement prompt to meaningfully correct itself since the sequential performance of the model is larger than the parallel performance above this means that rise indeed does this successfully 66 error analysis of rise over turns following the protocol of huang et al 21 in this section we perform an error analysis of the improve ment performed by rise without any oracle feedback to understand how the fraction of incorrect and correct responses changes over turns when no oracle is used for early termination we demonstrate this in the form of venn diagrams in figure 9 first note that there is a consistent increase in the portion of problems that stay correct and a consistent decrease in the portion of problems that stay incorrect which means that the model is able to answer more and more problems as we increase the number of turns second there is a consistent decrease in the number of problems that change from being correct to incorrect which is often also not the case for strong proprietary llms such as gpt in huang et al21 we also note that there is a decrease in the total number of incorrect problems that become correct in the subsequent turn but this is a direct consequence of a shrinkage in the size of the incorrect response set as more problems become correct over turns this indicates that one can induce intrinsic selfimprovement per the terminology of huang et al 21 via finetuning with rise even though no external environment input is provided during evaluation figure9change in the fraction of responses that transition their correctness values over the course of multiturn rollouts from rise wo oracle observe that in general the fraction of correct correct responses increases incorrect  incorrect responses decreases and the fraction of correct incorrect responses also decreases indicating that rise wo any oracle is able to iteratively improve its responses qualitative examples we also inspect several examples from the gsm8k test set to qualitatively understand the behavior of rise over turns and observe different behavior patterns that we show in appendix b2 for instance the trained model may choose to completely rewrite its previous response if it is totally incorrect in order to get to the correct answer or make small edits if the previous response is mostly correct another interesting pattern we note is that the model implicitly has the ability to locate errors in previous responses and only refine the erroneous steps additionally the model is tolerant of noisy environmental feedback when there is no oracleassisted early termination 15",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3040,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "8f65ee70-0c42-43da-a523-19524756b796",
        "embedding": null,
        "metadata": {
            "page_label": "16",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e166147e-78ab-4838-a0dd-72e305dad2d5",
                "node_type": "4",
                "metadata": {
                    "page_label": "16",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "5a266bfa42f4a4f622c7f0ccc24148c22385a61d06637047947f29f0699d187a",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "75838de9-55af-4f7b-8137-1d0897389bf3",
                "node_type": "1",
                "metadata": {},
                "hash": "eb0bf0c665a48c51d263dced362eb8661ef1bbdb2f84070bd5f99cb8f8cc4df5",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 7 discussion future directions and limitations we presented rise an approach for finetuning llms to be able to improve their own responses over multiple turns sequentially rise prescribes an iterative rl recipe on top of onpolicy rollout data with expert or selfgenerated supervision to steer selfimprovement rise significantly improves the self improvement abilities of 7b models on reasoning tasks gsm8k and math attaining an improvement over turns that previous work  21 has not observed in strong proprietary models in addition rise outperforms prior approaches that attempt to tackle similar problems of refinement and correction while being simpler in that it does not require running multiple models and works well with just one model despite these good results there are still many open questions and limitations due to computational constraints we were not able to perform more than two iterations of training with rise and no more than one iteration when the supervision comes from the learner itself improving with selfgenerated supervision will likely require more computation and more iterations since it will be slower than when using an offtheshelf expert model rise requires running manual iterations and hence a more online variant of rise is likely the solution in the long run especially when we wish to scale onpolicy learning in a dataefficient manner additionally while our work finetunes models on one task at a time it will be certainly interesting to include data from the protocols specified by rise into general instruction tuning and posttraining pipelines given the results that finetuning on data prescribed by rise does not hurt the firstturn performance of any model we trained we hypothesize that adding this sort of data in generalinstructiontuningpipelinesshouldnothurteitherwhileenablingthesequentialselfimprovement capability that is largely absent from models today acknowledgements this work was done at cmu we thank fahim tajwar abitha thankaraj amrith setlur and charlie snell for their feedback and informative discussions this work was supported by onr under n000142412206 openai superalignment fast grants and used the delta system and jetstream2  16 at the national center for supercomputing applications through cis240249 and cis230278 supported by the national science foundation we thank openai for providing gpt4 credits for academic use references 1rishabh agarwal nino vieillard piotr stanczyk sabela ramos matthieu geist and olivier bachem gkd generalized knowledge distillation for autoregressive sequence models arxiv preprint arxiv230613649  2023 2yuntao bai saurav kadavath sandipan kundu amanda askell jackson kernion andy jones anna chen anna goldie azalia mirhoseini cameron mckinnon et al constitutional ai harmlessness from ai feedback arxiv preprint arxiv221208073  2022 3collin burns pavel izmailov jan hendrik kirchner bowen baker leo gao leopold aschenbrenner yining chen adrien ecoffet manas joglekar jan leike ilya sutskever and jeff wu weakto strong generalization eliciting strong capabilities with weak supervision 2023 url https arxivorgabs231209390  4jonathan d chang wenhao shan owen oertell kiant\u00e9 brantley dipendra misra jason d lee and wen sun dataset reset",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3305,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "75838de9-55af-4f7b-8137-1d0897389bf3",
        "embedding": null,
        "metadata": {
            "page_label": "16",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e166147e-78ab-4838-a0dd-72e305dad2d5",
                "node_type": "4",
                "metadata": {
                    "page_label": "16",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "5a266bfa42f4a4f622c7f0ccc24148c22385a61d06637047947f29f0699d187a",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "8f65ee70-0c42-43da-a523-19524756b796",
                "node_type": "1",
                "metadata": {
                    "page_label": "16",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "0a066f432e79fa3cab24ce9c3c99923baf0cbffd573d049c48f4c638930fa81e",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "weak supervision 2023 url https arxivorgabs231209390  4jonathan d chang wenhao shan owen oertell kiant\u00e9 brantley dipendra misra jason d lee and wen sun dataset reset policy optimization for rlhf arxiv preprint arxiv240408495  2024 16",
        "mimetype": "text/plain",
        "start_char_idx": 3140,
        "end_char_idx": 3373,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "b6c86237-5573-4c6b-8f35-24b81a4ede99",
        "embedding": null,
        "metadata": {
            "page_label": "17",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ac803eb9-c0dd-42fa-a1c5-5e99d613cd8d",
                "node_type": "4",
                "metadata": {
                    "page_label": "17",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "06900f0149dfcf36c558d77a06113c65291ce45394a76dac0e971eed09229275",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "226d03d7-2014-424f-a672-4a40fe91aff2",
                "node_type": "1",
                "metadata": {},
                "hash": "1671d5ee454ac7f6a8568639640149af9f78e9300462fe2ae335657ac651137a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 5yiannis charalambous norbert tihanyi ridhi jain youcheng sun mohamed amine ferrag and lucas c cordeiro a new era in software security towards selfhealing software via large language models and formal verification arxiv preprint arxiv230514752  2023 6baian chen chang shu ehsan shareghi nigel collier karthik narasimhan and shunyu yao fireact toward language agent finetuning 2023 7xinyun chen maxwell lin nathanael sch\u00e4rli and denny zhou teaching large language models to selfdebug arxiv preprint arxiv230405128  2023 8zixiang chen yihe deng huizhuo yuan kaixuan ji and quanquan gu selfplay finetuning converts weak language models to strong language models arxiv preprint arxiv240101335  2024 9yew ken chia guizhen chen luu anh tuan soujanya poria and lidong bing contrastive chainofthought prompting arxiv preprint arxiv231109277  2023 10karlcobbe christopherhesse jacobhilton andjohnschulman leveragingproceduralgeneration to benchmark reinforcement learning arxiv preprint arxiv191201588  2019 11karlcobbevineetkosarajumohammadbavarianmarkchenheewoojunlukaszkaisermatthias plappert jerry tworek jacob hilton reiichiro nakano christopher hesse and john schulman training verifiers to solve math word problems arxiv preprint arxiv211014168  2021 12yilun du shuang li antonio torralba joshua b tenenbaum and igor mordatch improving factual ity and reasoning in language models through multiagent debate arxiv preprint arxiv230514325  2023 13kanishk gandhi denise lee gabriel grand muxin liu winson cheng archit sharma and noah d goodman stream of search sos learning to search in language arxiv preprint arxiv240403683  2024 14luyu gao aman madaan shuyan zhou uri alon pengfei liu yiming yang jamie callan and graham neubig pal programaided language models in international conference on machine learning  pages 1076410799 pmlr 2023 15zhibin gou zhihong shao yeyun gong yelong shen yujiu yang nan duan and weizhu chen critic large language models can selfcorrect with toolinteractive critiquing arxiv preprint arxiv230511738  2023 16david y hancock jeremy fischer john michael lowe winona snappchilds marlon pierce suresh marru j eric coulter matthew vaughn brian beck nirav merchant edwin skidmore and gwen jacobs jetstream2 accelerating cloud computing via jetstream in practice and experience in advanced research computing  pearc 21 new york ny usa 2021 association for computing machinery isbn",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2472,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "226d03d7-2014-424f-a672-4a40fe91aff2",
        "embedding": null,
        "metadata": {
            "page_label": "17",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ac803eb9-c0dd-42fa-a1c5-5e99d613cd8d",
                "node_type": "4",
                "metadata": {
                    "page_label": "17",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "06900f0149dfcf36c558d77a06113c65291ce45394a76dac0e971eed09229275",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "b6c86237-5573-4c6b-8f35-24b81a4ede99",
                "node_type": "1",
                "metadata": {
                    "page_label": "17",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "0af4024caa6aaf2dc9b0ce34988ec057c7474512c0242560ea92b95b5239720a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "brian beck nirav merchant edwin skidmore and gwen jacobs jetstream2 accelerating cloud computing via jetstream in practice and experience in advanced research computing  pearc 21 new york ny usa 2021 association for computing machinery isbn 9781450382922 doi 10114534373593465565 url https doiorg10114534373593465565  17alex havrilla sharath raparthy christoforus nalmpantis jane dwivediyu maksym zhuravinskyi eric hambro and roberta railneau glore when where and how to improve llm reasoning via global and local refinements arxiv preprint arxiv240210963  2024 17",
        "mimetype": "text/plain",
        "start_char_idx": 2232,
        "end_char_idx": 2796,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "84419362-68fc-4705-bf9c-83d08d091e0a",
        "embedding": null,
        "metadata": {
            "page_label": "18",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "d3116860-d44d-46a0-a7ca-169e438704af",
                "node_type": "4",
                "metadata": {
                    "page_label": "18",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "91276cfdc8aaef4847d4e5a652009fde8034420e284f61322aa662228b04c3b3",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "bcb3bf6c-cddb-4ff0-95db-1c3900a4daea",
                "node_type": "1",
                "metadata": {},
                "hash": "6da195f6ae177ccfd94867d533cf5bcf97d961b9ea8fa90b3fac5dda38ff4665",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 18dan hendrycks collin burns saurav kadavath akul arora steven basart eric tang dawn song and jacob steinhardt measuring mathematical problem solving with the math dataset neurips  2021 19arian hosseini xingdi yuan nikolay malkin aaron courville alessandro sordoni and rishabh agarwal vstar training verifiers for selftaught reasoners arxiv preprint arxiv240206457  2024 20dong huang qingwen bu jie m zhang michael luck and heming cui agentcoder multiagent based code generation with iterative testing and optimisation arxiv preprint arxiv231213010  2023 21jie huang xinyun chen swaroop mishra huaixiu steven zheng adams wei yu xinying song and denny zhou large language models cannot selfcorrect reasoning yet arxiv preprint arxiv231001798  2023 22wenlong huang pieter abbeel deepak pathak and igor mordatch language models as zeroshot planners extracting actionable knowledge for embodied agents in international conference on machine learning  pages 91189147 pmlr 2022 23albert q jiang alexandre sablayrolles arthur mensch chris bamford devendra singh chaplot diego de las casas florian bressand gianna lengyel guillaume lample lucile saulnier et al mistral 7b arxiv preprint arxiv231006825  2023 24katie kang eric wallace claire tomlin aviral kumar and sergey levine unfamiliar finetuning examples control how language models hallucinate 2024 25diederik p kingma and max welling autoencoding variational bayes 2022 url https arxivorgabs13126114  26lucas lehnert sainbayar sukhbaatar paul mcvay michael rabbat and yuandong tian be yond a better planning with transformers via search dynamics bootstrapping arxiv preprint arxiv240214083  2024 27chen li weiqi wang jingcheng hu yixuan wei nanning zheng han hu zheng zhang and houwenpeng common7blanguagemodelsalreadypossessstrongmathcapabilities arxivpreprint arxiv240304706  2024 28hunter lightman vineet kosaraju yura burda harri edwards bowen baker teddy lee jan leike john schulman ilya sutskever and karl cobbe lets verify step by step arxiv preprint arxiv230520050  2023 29xiaoliu haoyu hanchenzhang yifanxu xuanyulei hanyulai yugu hangliangding kaiwen men kejuan yang et al agentbench evaluating llms as agents arxiv preprint arxiv230803688  2023 30haipengluoqingfengsuncanxupuzhaojianguanglouchongyangtaoxiubogengqingwei lin shifeng chen and dongmei zhang wizardmath empowering mathematical reasoning for large language models via reinforced evolinstruct arxiv preprint",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2500,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "bcb3bf6c-cddb-4ff0-95db-1c3900a4daea",
        "embedding": null,
        "metadata": {
            "page_label": "18",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "d3116860-d44d-46a0-a7ca-169e438704af",
                "node_type": "4",
                "metadata": {
                    "page_label": "18",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "91276cfdc8aaef4847d4e5a652009fde8034420e284f61322aa662228b04c3b3",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "84419362-68fc-4705-bf9c-83d08d091e0a",
                "node_type": "1",
                "metadata": {
                    "page_label": "18",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "4155aec13d995d46bca5218002da4c4d6055ebf1602e24878e548f936c835d34",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "lin shifeng chen and dongmei zhang wizardmath empowering mathematical reasoning for large language models via reinforced evolinstruct arxiv preprint arxiv230809583  2023 18",
        "mimetype": "text/plain",
        "start_char_idx": 2352,
        "end_char_idx": 2524,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "326e7fef-6dd9-421f-9c54-9615240ce09c",
        "embedding": null,
        "metadata": {
            "page_label": "19",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e701c5fa-d0aa-4e19-bcde-d58aaa01cf61",
                "node_type": "4",
                "metadata": {
                    "page_label": "19",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "79a895ade6e0d594dd97b6e10af7566594d876d0d1f58f24f6315e4506c5fecc",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "6ce101f0-e23f-4692-ae4b-d9920e0189e2",
                "node_type": "1",
                "metadata": {},
                "hash": "24569870418417553da126a1f77b56acb3e4153eacdecb416222fade251ea9c4",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 31aman madaan niket tandon prakhar gupta skyler hallinan luyu gao sarah wiegreffe uri alon nouha dziri shrimai prabhumoye yiming yang et al selfrefine iterative refinement with selffeedback arxiv preprint arxiv230317651  2023 32erik nijkamp bo pang hiroaki hayashi lifu tu huan wang yingbo zhou silvio savarese and caiming xiong codegen an open large language model for code with multiturn program synthesis iclr 2023 33maxwell nye anders johan andreassen guy gurari henryk michalewski jacob austin david bieber david dohan aitor lewkowycz maarten bosma david luan et al show your work scratchpads for intermediate computation with language models arxiv preprint arxiv211200114  2021 34xue bin peng aviral kumar grace zhang and sergey levine advantageweighted regression simple and scalable offpolicy reinforcement learning arxiv preprint arxiv191000177  2019 35janpetersandstefanschaal reinforcementlearningbyrewardweightedregressionforoperational spacecontrol in proceedingsofthe24thinternationalconferenceonmachinelearning pages745750 acm 2007 36stephanerossgeoffreygordonanddrewbagnell areductionofimitationlearningandstructured prediction to noregret online learning in geoffrey gordon david dunson and miroslav dud\u00edk editors proceedings of the fourteenth international conference on artificial intelligence and statistics  volume 15 of proceedings of machine learning research  pages 627635 fort lauderdale fl usa 1113 apr 2011 pmlr url httpproceedingsmlrpressv15ross11ahtml  37corby rosset chingan cheng arindam mitra michael santacroce ahmed awadallah and tengyang xie direct nash optimization teaching language models to selfimprove with general preferences arxiv preprint arxiv240403715  2024 38swarnadeep saha omer levy asli celikyilmaz mohit bansal jason weston and xian li branchsolvemerge improves large language model evaluation and generation arxiv preprint arxiv231015123  2023 39timo schick jane dwivediyu roberto dess\u00ec roberta raileanu maria lomeli luke zettlemoyer nicola cancedda and thomas scialom toolformer language models can teach themselves to use toolsarxiv preprint arxiv230204761  2023 40noah shinn beck labash and ashwin gopinath reflexion an autonomous agent with dynamic memory and selfreflection arxiv preprint arxiv230311366  2023 41charlie snell ilya kostrikov yi su mengjiao yang and sergey levine offline rl for natural language generation with implicit language q learning arxiv preprint arxiv220611871  2022 42jaschasohldicksteinericaweissnirumaheswaranathanandsuryaganguli deepunsupervised learning using nonequilibrium",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2632,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "6ce101f0-e23f-4692-ae4b-d9920e0189e2",
        "embedding": null,
        "metadata": {
            "page_label": "19",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e701c5fa-d0aa-4e19-bcde-d58aaa01cf61",
                "node_type": "4",
                "metadata": {
                    "page_label": "19",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "79a895ade6e0d594dd97b6e10af7566594d876d0d1f58f24f6315e4506c5fecc",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "326e7fef-6dd9-421f-9c54-9615240ce09c",
                "node_type": "1",
                "metadata": {
                    "page_label": "19",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "9db7d1297db8b3a7b57b3a8cae246a490d8e52465d53641714d1c9e6008221f2",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "language q learning arxiv preprint arxiv220611871  2022 42jaschasohldicksteinericaweissnirumaheswaranathanandsuryaganguli deepunsupervised learning using nonequilibrium thermodynamics 2015 url httpsarxivorgabs1503 03585 43yang song and diederik p kingma how to train your energybased models 2021 url https arxivorgabs210103288  19",
        "mimetype": "text/plain",
        "start_char_idx": 2464,
        "end_char_idx": 2794,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "35e76ccd-66ee-4b81-8fd7-a7becc47619d",
        "embedding": null,
        "metadata": {
            "page_label": "20",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a5e9e894-eeed-4ece-86ea-f5ba5b1dc433",
                "node_type": "4",
                "metadata": {
                    "page_label": "20",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "b02e39d7db80d42629a1ffc9eeb00bb6e267f8ddff00e83eb62bbb704d3e6ea0",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "680c5a44-fc0f-4135-af7e-9ed469132277",
                "node_type": "1",
                "metadata": {},
                "hash": "a0c6cc0b64f932c71aa564814ed6f5007ada9245ce30d00d3e3729318ea9c6db",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 44liting sun cheng peng wei zhan and masayoshi tomizuka a fast integrated planning and control framework for autonomous driving via imitation learning in dynamic systems and control conference  volume 51913 page v003t37a012 american society of mechanical engineers 2018 45gokul swamy sanjiban choudhury j andrew bagnell and zhiwei steven wu inverse reinforce ment learning without reinforcement learning 2024 url httpsarxivorgabs2303 14623 46shubham toshniwal ivan moshkov sean narenthiran daria gitman fei jia and igor gitman openmathinstruct1 a18millionmathinstructiontuningdataset arxivpreprintarxiv240210176  2024 47jonathan uesato nate kushman ramana kumar francis song noah siegel lisa wang antonia creswell geoffrey irving and irina higgins solving math word problems with processand outcomebased feedback arxiv preprint arxiv221114275  2022 48boshi wang sewon min xiang deng jiaming shen you wu luke zettlemoyer and huan sun towards understanding chainofthought prompting an empirical study of what matters arxiv preprint arxiv221210001  2022 49guanzhi wang yuqi xie yunfan jiang ajay mandlekar chaowei xiao yuke zhu linxi fan and anima anandkumar voyager an openended embodied agent with large language models arxiv preprint arxiv arxiv230516291  2023 50peiyi wang lei li zhihong shao rx xu damai dai yifei li deli chen y wu and zhifang sui mathshepherd verify and reinforce llms stepbystep without human annotations corr abs231208935  2023 51xuezhiwangjasonweidaleschuurmansquocleedchisharannarangaakankshachowdhery and denny zhou selfconsistency improves chain of thought reasoning in language models arxiv preprint arxiv220311171  2022 52jason wei xuezhi wang dale schuurmans maarten bosma fei xia ed chi quoc v le denny zhou et al chainofthought prompting elicits reasoning in large language models neurips  2022 53sean welleck ximing lu peter west faeze brahman tianxiao shen daniel khashabi and yejin choi generating sequences by learning to selfcorrect in the eleventh international conference on learning representations  2023 url httpsopenreviewnetforumidhh36jeqzdao  54hui yang sifu yue and yunzhong he autogpt for online decision making benchmarks and additional opinions arxiv preprint arxiv230602224  2023 55kaiyu yang aidan m swope alex gu rahul chalamala peiyang song shixing yu saad godil ryan prenger and anima anandkumar leandojo theorem proving with retrievalaugmented language models arxiv preprint arxiv230615626  2023",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2520,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "680c5a44-fc0f-4135-af7e-9ed469132277",
        "embedding": null,
        "metadata": {
            "page_label": "20",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a5e9e894-eeed-4ece-86ea-f5ba5b1dc433",
                "node_type": "4",
                "metadata": {
                    "page_label": "20",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "b02e39d7db80d42629a1ffc9eeb00bb6e267f8ddff00e83eb62bbb704d3e6ea0",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "35e76ccd-66ee-4b81-8fd7-a7becc47619d",
                "node_type": "1",
                "metadata": {
                    "page_label": "20",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "8e9e05636ec2fc24a8d75c6fb611095dd43d7b09d2acbd23c9918aea608d6081",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "peiyang song shixing yu saad godil ryan prenger and anima anandkumar leandojo theorem proving with retrievalaugmented language models arxiv preprint arxiv230615626  2023 56shunyu yao jeffrey zhao dian yu nan du izhak shafran karthik narasimhan and yuan cao react synergizing reasoning and acting in language models arxiv preprint arxiv221003629  2022 20",
        "mimetype": "text/plain",
        "start_char_idx": 2351,
        "end_char_idx": 2704,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f1532d42-28bc-46b4-bedc-29494e04527c",
        "embedding": null,
        "metadata": {
            "page_label": "21",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "903a9339-fb2a-46fc-b2c1-532352e2cf12",
                "node_type": "4",
                "metadata": {
                    "page_label": "21",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "c26b80e36117e47fc41a358c336c4126ae0392aa1f35437b5afb2d49ea9b5d2a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove 57longhuiyu weisenjiang hanshi jinchengyu zhengyingliu yuzhang jamestkwok zhenguo li adrian weller and weiyang liu metamath bootstrap your own mathematical questions for large language models arxiv preprint arxiv230912284  2023 58lifan yuan ganqu cui hanbin wang ning ding xingyao wang jia deng boji shan huimin chen ruobing xie yankai lin et al advancing llm reasoning generalists with preference trees arxiv preprint arxiv240402078  2024 59 weizhe yuan richard yuanzhe pang kyunghyun cho sainbayar sukhbaatar jing xu and jason weston selfrewarding language models arxiv preprint arxiv240110020  2024 60xiang yue xingwei qu ge zhang yao fu wenhao huang huan sun yu su and wenhu chen mammoth building math generalist models through hybrid instruction tuning arxiv preprint arxiv230905653  2023 61eric zelikman yuhuai wu jesse mu and noah goodman star bootstrapping reasoning with reasoning advances in neural information processing systems  351547615488 2022 62aohan zeng mingdao liu rui lu bowen wang xiao liu yuxiao dong and jie tang agenttuning enabling generalized agent abilities for llms arxiv preprint arxiv231012823  2023 63tianjun zhang xuezhi wang denny zhou dale schuurmans and joseph e gonzalez tempera testtime prompting via reinforcement learning arxiv preprint arxiv221111890  2022 64tianjun zhang aman madaan luyu gao steven zheng swaroop mishra yiming yang niket tandon and uri alon incontext principle learning from mistakes arxiv preprint arxiv240205403  2024 65andy zhou kai yan michal shlapentokhrothman haohan wang and yuxiong wang lan guage agent tree search unifies reasoning acting and planning in language models arxiv preprint arxiv231004406  2023 66yifei zhou andrea zanette jiayi pan sergey levine and aviral kumar archer training language model agents via hierarchical multiturn rl arxiv preprint arxiv240219446  2024 21",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1922,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "25e0acbf-04e0-4e96-9e00-9f2ae5e93ecb",
        "embedding": null,
        "metadata": {
            "page_label": "22",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "61c83860-8b47-49d1-b831-2165cc5c7bb3",
                "node_type": "4",
                "metadata": {
                    "page_label": "22",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "a028e8ce170f29bd96a255cd755c2dc579ff69324998858b9989301a0a3281a8",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove appendices a additional ablations on data composition and weaktostrong generalization a1 inclusion of correcttocorrect data intuitively selfimprovement over turns is largely only possible when the model can learn to verify the correctness of its previous response and decide to appropriately modify its response toward correctness thus far the rise has only trained on data that showed how to convert incorrect responses to correct responses but never illustrated how the model could act on correct responses to understand if perfor mance can be boosted by also illustrating examples of how the model could act on correct responses we ran a number of ablations we took the rise data generated during iteration 1 of training on gsm8k with llama27b and modified the multiturn rollouts to create several cases first we duplicated the correct response appearing at the end of every successful multiturn rollout and trained for one extra turn this should teach the model that correct responses should not be modified unlike incorrect responses appearing in previous turns in the rollout second we also ran a variant in which the correct response appearing at the end of every successful rollout is followed by a different correct response this variant should teach the model that if it chooses to modify a correct response it must still produce another correct response as shown in table 4 all methods improved performance over the base model though only appending with a successful rollout with a novel correct response leads to best performance the default design of rise in the main paper attains a close second position and repeating a correct response at the end of a successful rollout largely reduces performance we suspect that the poor performance of repeating the same correct response is largely a result of inducing spurious correlations due to data duplication rise llama2wo oracle w oracle m1t1m5t1m1t5 p1t5 boost 329 453 124 26564 40980 rise default 356 497 141 507151 639283 repeating a correct response 342 489 146 462126 577235 appending a different correct response 331 493 162 511180 649318 table 4 comparison of model performance on gsm8k with different mechanisms of adding correcttocorrect data in risevalues in parentheses indicate improvement over m1t1 note that appending a successful rollout with a a novel correct response leads to the highest performance gains to further investigate selfimprovement capabilities we analyzed the percentage of correct responses changing to incorrect responses in consecutive turns t \ud835\udc56to t\ud835\udc56 1 as illustrated in figure 10 generally a decreasing trend suggests better selfimprovement while lower absolute values indicate better resistance to noisy feedback the results reveal unexpected patterns across configurations the boost configuration shows the poorest performance with the highest overall percentages and an increase from turn 4 to 5 suggesting that it struggles to consistently maintain correct responses repeating a correct response shows the lowest initial percentage 63 but increases from turn 3 onward indicating potential issues in extended interactions both default rise and appending a different correct response demonstrate a favorable trend steadily decreasing from 123 to 39 and from 98 to 33 respectively suggesting a good balance between maintaining correct responses and allowing improvements these findings provide nuanced insights into the stability and selfimprovement capabilities of rise and align with our earlier observation of its superior performance in overall accuracy 22",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3630,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "d09f80a4-4f9a-4203-8a3d-d54ee3558224",
        "embedding": null,
        "metadata": {
            "page_label": "23",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e98b1ca0-4f2c-4090-86cc-10315bad08d0",
                "node_type": "4",
                "metadata": {
                    "page_label": "23",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "5367795c5d5e08205ce3cd5e8f923cf59514ac75864ea800bc95e6650fd520f3",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove figure10percentage of correct responses in turn t \ud835\udc56that change to being incorrect in turn t \ud835\udc561this figure illustrates the percentage of correct responses that change to incorrect responses across consecutive turns t \ud835\udc56to t\ud835\udc561 for different model configurations a continuously decreasing trend suggests better selfimprovement performance a2 weaktostrong generalization rise on weak model data improves strong models in this section we compare the performance of llama2 and mistral7b with rise in the weaktostrong setting 3 concretelyweareinterestedinusingdatageneratedviarisewithaweakmodelllama27b to train a strong model mistral7b our analysis reveals intriguing insights into the transferability of risegenerated data across models of different capabilities risewo oracle w oracle m1t1m5t1m1t5 p1t5 llama27b 105 228 123 11106 13934  iteration 1 356 497 141 507151 639283  iteration 1 mistral7b 271 401 130 452181 591320 mistral7b 337 494 157 39053 469132  iteration 1 353 506 153 592239 686333  iteration 1 llama27b 382 554 172 627245 735353 table 5 weaktostrong generalization on gsm8k comparing performance of rise when training on rollouts generated by llama27b vs mistral7b note that training the mistral7b model on rollouts generated by the weaker llama27b with rise improves performance compared to using data generated by the mistral7b model itself however the reverse is not true training the llama2 model on mistrals mistakes leads to worse performance likely because errors from the mistral7b model are harder to comprehend for a worse base model all values are in  accuracy and values in parentheses indicate improvement over m1t1 as shown in table 5 we find that mistral7b  iteration 1 data generated from llama2 outperforms training the llama27b model itself on these data ie llama27b  iteration1 on all the metrics reported with particularly significant improvements in multiturn reasoning m1t5 in fact training on multiturn rollouts from llama27b also outperforms training on onpolicy mistral7b rollouts as well interestingly we observed that training llama27b on multiturn rollouts from mistral7b performs worse than training on onpolicy llama27b rollouts suggesting that llama27b despite its lower absolute performance demonstrates more informative mistakes that can be leveraged to better boost the self improvement capability this phenomenon underscores the importance of the quality and nature of errors in the training data rather than just the overall performance of the model that generates them these findings collectively suggest that the data generated from a weaker llama2 model can still be used to induce a selfimprovement capability in a stronger model although the reverse is not true as is also evident from the fact that using gpt35 rollouts in the boosting phase for training does not improve performance for any model in table 1 we suspect that this is becaue the reverse poses a much harder 23",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3000,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "5eae1dc5-4bf0-4da8-b361-a0f7a7f7f945",
        "embedding": null,
        "metadata": {
            "page_label": "24",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f3171e38-188a-44f4-9ea0-c9635903ef30",
                "node_type": "4",
                "metadata": {
                    "page_label": "24",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "b33455386445153cbad820f5f0d4bdcf4c78427707a73d40338a87b0e1ded248",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "aa66a863-b82a-477e-bef1-ad05ceb69ad4",
                "node_type": "1",
                "metadata": {},
                "hash": "5cbd0521b10fe0b496275e511349766fb177e1bc254449faa5cfb9e9cc17cb06",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove learning problem since a weak model need to internalize the mistakes of a stronger model resulting in hallucinations and memorization  24 note that training on these data does not degrade singleturn performance either this hints at an added benefit of training with rise weaktostrong generalization which can be quite useful in practice when rolling out stronger models is expensive b additional results b1 complete comparisons and discussion extended version of table 1 we provide an extended version of table 1 with a clear explanation of how we implement baselines and a discussion of comparisons approachgsm8k 10 math18 wo oracle w oracle wo oracle w oracle m1t1m5t1m1t5 p1t5 m1t1m5t1m1t5 p1t5 rise ours llama2 base 105 228 123 11106 13934 19 51 32 1405 2304 boost 329 454 125 39263 555226 55 68 13 5500 14691 iteration 1 356 497 141 507151 639283 63 88 25 9734 194131 iteration 2 373 510 137 550177 684311 58 104 46 10446 198140 rise ours mistral7b 337 494 157 39053 469132 75 130 55 8409 13055  iteration 1 353 506 153 592239 686333 67 95 28 184111 297224 sft on oracle data only correct data 274 422 149 34066 436162 58 79 21 5503 12162 correct and incorrect 257 418 161 31255 415158 50 52 02 5000 13181 baselines gpt35 664 806 142 71046 74783 397 478 81 45154 543146 mistral7b 337 494 157 39053 469132 75 130 55 8409 13055 eurus7bsft 363 663 300 479116 531168 123 198 75 16340 229106 selfrefine m1t3p1t3 m1t3p1t3 base 105 224 119 7134 13025 19 51 32 1900 3112 boost 329 453 124 26564 40980 55 65 10 2926 7217 iteration1 356 495 139 31739 43781 63 87 24 5904 9936 iteration2 373 505 132 33340 44572 58 94 36 5701 9537 gpt35 664 802 138 61054 71652 397 465 68 36532 46770 mistral7b 337 485 148 212125 37942 75 123 48 7104 11439 eurus7bsft 363 659 296 262101 42865 123 194 71 9033 15128 glore m1t3p1t3 orm 482 495 13 57189 sorm",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1904,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "aa66a863-b82a-477e-bef1-ad05ceb69ad4",
        "embedding": null,
        "metadata": {
            "page_label": "24",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f3171e38-188a-44f4-9ea0-c9635903ef30",
                "node_type": "4",
                "metadata": {
                    "page_label": "24",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "b33455386445153cbad820f5f0d4bdcf4c78427707a73d40338a87b0e1ded248",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "5eae1dc5-4bf0-4da8-b361-a0f7a7f7f945",
                "node_type": "1",
                "metadata": {
                    "page_label": "24",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "788f12b837ee18ef39a9c06c28b952f49d9421fd5ac29fe4b9c5c4b07819ad06",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "363 659 296 262101 42865 123 194 71 9033 15128 glore m1t3p1t3 orm 482 495 13 57189 sorm 482 516 34 597115 na direct 482 474 08 592110 vstar m64t1 star 280 461 181 verification 280 562 282 na vstar 280 632 352 table 6 comparing rise with other approaches selfrefine glore and vstar and other baseline approaches observe that rise attains the biggest performance improvements between 1turn and 5turn performance without the use of an oracle on both gsm8k and math this performance gap is even larger when oracle early termination is allowed 5turn w oracle selfrefine largely degrades performance across the board glore trains a separate refinement model but still performs worse than rise 24",
        "mimetype": "text/plain",
        "start_char_idx": 1817,
        "end_char_idx": 2506,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f3f1e45d-a4c2-4f37-afcb-ac87c8e837de",
        "embedding": null,
        "metadata": {
            "page_label": "25",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "7d47e63a-1181-4912-aa7a-b1165a51f369",
                "node_type": "4",
                "metadata": {
                    "page_label": "25",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "1b1743333f10ec4bfea97370d276dfbc996b85ac218fac5d5f15e13e44c8cd07",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove comparison with selfrefine  31to build a selfrefine baseline  31 evaluation we slightly modified our evaluation pipeline following the selfrefine approach in this setup figure 11 the model generates an initial response and then the environment prompts the model to locate errors in the generated solution and refine its answer based on the initial response and the identified error selfrefine system you are an ai language model designed to assist with math problemsolving in this task i will provide you with math problems your goal is to solve the problem stepbystep showing your reasoning at each step after you have finished solving the problem present your final answer as boxedyour answer oneshot example 17  userquery  agentinitial answer  user there is an error in the solution above because of lack of understanding of the question what istheerror tofindtheerrorgothrougheachstepofthesolutionandcheckifeverythinglooksgood agentcritic  user now rewrite the solution in the required format agentrefined answer  figure11prompt for selfrefine  we follow the standard pipeline of the original paper prompt the llm to refine and correct its previous mistakes however our experiments show that without any oracle hint from the environment or human feedback the selfrefine approach leads to a degradation in performance across all models only when oracle feedback is available to assist with early termination does the selfrefine approach provide a slight performance boost this highlights the limitation of the selfrefine structure in effectively improving model performance without external guidance which is also observed in 22 in contrast the model trained with rise can attain consistent performance improvements without relying on an oracle by training the model to iteratively refine its responses our method enables the model to selfcorrect and improve its performance over multiple turns this showcases the effectiveness of our approach incomparison to the selfrefine baseline as it allows for more robust and consistent performance gains without the need for the oracle assistance comparison with glore  17glore is a multimodel system that relies on a student model to propose drafts an outcomebased reward model orm or stepwise orm to locate errors at different granularity levels and a global or local refinement model for adjusting these errors since no code was openly available for this approach in our experiments we compared to the numbers from the main paper havrilla et al 17 while the comparison against glore is already applestooranges since our 25",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2642,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "2c770c8b-acc2-4afa-91ca-af8e03b3475f",
        "embedding": null,
        "metadata": {
            "page_label": "26",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2313ba3c-e132-4178-8a6a-6abb9e5a5be5",
                "node_type": "4",
                "metadata": {
                    "page_label": "26",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "6bd2ba1394d6579edf92f5bbbaff10e0727074e3a92e7f0e364cdb7925e79994",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove method only trains a single endtoend model while glore trains multiple models performancewise glores global and local refinement models show little to no improvement in overall accuracy without an oracle and even exhibit decreasing accuracy in some cases however when an oracle is used to guide the refinement process glore demonstrates a 10 improvement on the 7b model in the gsm8k dataset as anticipated since we run rise from a less advanced base model llama2 7b we observe a slightly lower absolute performance compared to glore however rise demonstrates its effectiveness in self improvement by sequentially enhancing its performance by an impressive 134 within just 3 turns without an oracle feedback and by a remarkable 234 with an oracle on gsm8k this showcase of rises capabilities is particularly noteworthy considering that glore utilizes 3 independent models  one for generating candidate solutions one reward model for locating errors and one refinement model for refinement comparison with vstar  19vstar requires training an additional verifier model to rank candidate answers generated by the targeted model but it does not make any sequential revisions or improvements to a response while comparing rise to using a verifier for reranking the top 5 responses at the first turn as a base comparison would have been informative we were unable to find this specific result in the original vstar paper the results presented in the official table 6 for vstar correspond to running 64 samples which improves the base models performance by 352 for each prompt during evaluation in contrast our method rise after the same amount of finetuning iterations 3 iterations and using only 5 samples improves upon the base model by 445 calculated as 550  105  445 this comparison highlights rises efficiency in achieving significant improvements with fewer samples and iterations compared to vstars approach of using a large number of samples without sequential refinement moreover vstars performance is inherently bounded by the candidate generators performance as discussed in section 5 if there is no correct response among the generated candidates the problem remains unsolved in contrast we show in figure 6 that rise can also solve problems that were not solved by majority voting with a much higher budget in the first turn furthermore we believe that combining vstar with rise could lead to even better performance as rise can generate better models and a verifier can be complementarily used for filtering comparison with other base models mistral7b  23 and eurus7bsft  58 are models that exhibit comparable performance to our method in terms of the absolute maj5 performance however it is crucial to note that these base models are finetuned using a vast amount of data including data specifically tuned for math reasoning performance  58 while our model is finetuned on a single domain that said we do show that finetuning with rise can still enhance the performance of mistral7b models to summarize our method offers several advantages over glore and vstar such as endtoend error correction with a single model superior performance with fewer samples and the ability to solve problems that cannot be solved by random sampling in the first turn although our maj1 performance is lower than glores base model ei which is an applestooranges comparison our ultimate 5turn performance surpasses their best absolute performance in both oracle and nonoracle scenarios compared to other base models like mistral7b and eurus7bsft our method achieves comparable performance while being finetuned on a single domain and it can be generalized to better base models 26",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3738,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f603a60f-83b0-43de-9d2c-d7634d4b9a9a",
        "embedding": null,
        "metadata": {
            "page_label": "27",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "4f55fc47-edeb-4a43-a4e1-a5f4a3a756e9",
                "node_type": "4",
                "metadata": {
                    "page_label": "27",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "27b4b04fad3068387b36763d89d0aaf7207433a1444294ad287f307f7c3889ce",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove b2 qualitative examples from rise in this section we provide some qualitative examples showing the nature of edits made by rise on various datasets that we train on we utilize the model from iteration 2 of rise 27",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 285,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "ea17c2d3-8463-483d-9c33-e145deb8a918",
        "embedding": null,
        "metadata": {
            "page_label": "28",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "dbfb962f-8dcb-4d2c-98a2-b0c37a1f9628",
                "node_type": "4",
                "metadata": {
                    "page_label": "28",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "9cce37f8ef414986b9392bf9e3c1caf4d73f8cb8b3bd8576a8b413948a22686d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove rise example small edits query mike plays ping pong for 40 minutes in the first 20 minutes he scores 4 points in the second 20 minutes he scores 25 more points how many total points did he score turn1 response  step 1 in the first 20 minutes mike scores 4 points step 2 in the second 20 minutes mike scores 25 more points than he did in the first 20 minutes somikescores448pointsinthesecond20minutes step 3 to find the total points we need to add the points scored in the first and second 20 minutes total_points  4  8  12 answer 12 turn2 response  i apologize for the mistake lets solve the problem step by step step 1 in the first 20 minutes mike scores 4 points step 2 in the second 20 minutes mike scores 25 more points than he did in the first 20 minutes thismeanshescores4425415pointsinthesecond20minutes step 3 to find the total points we add the points scored in the first and second 20 minutes 4  5  9 points answer 9 rise example big edits query in a candy machine there are 22 more than four times the number of pink gumballs as there are blue gumballs if there are 12 blue gumballs how many pink ones are there turn1 response  step 1 lets assume the number of blue gumballs as x step 2 according to the given information there are 22 more than four times the number of pink gumballs as there are blue gumballs so the number of pink gumballs can be expressed as 4x  22 step3wearegiventhatthereare12blue gumballssoletssubstitutexwith12intheequation answer 54 turn2 response  lets solve the problem step by step step 1 lets denote the number of blue gumballs as x step 2 we are given that the number of pink gumballs is 22 more than four times the number of blue gumballs so the number of pink gumballs can be expressed as 4x  22 step3wearealsogiven thatthereare12bluegumballssowecansubstitutex12intotheexpressionforthenumberof pinkgumballs41222482270thereforethereare boxed70 pinkgumballs answer 70 figure12examples of rise correct its previous behavior in different modes some only make changes to a small part small edits some may directly rewrite most of its previous answers big edits because the first step in the previous answer is wrong the mistaken steps of different turns are highlighted in red and the correct are highlighted in green this demonstrates shows how rise can correct its previous answers and finally get to a correct answer 28",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2432,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e05e0237-c50e-43d2-96a8-72cd4f6ab74f",
        "embedding": null,
        "metadata": {
            "page_label": "29",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ab494178-bef0-47bd-af02-9f102a2534cd",
                "node_type": "4",
                "metadata": {
                    "page_label": "29",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "5ee8d46f27df2ca004a1db340c4b2fde2bf26c2ae5fa1b4e96aecf987092a09a",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "515ac710-f9f9-4f03-8e68-9ea3ba3cc830",
                "node_type": "1",
                "metadata": {},
                "hash": "74b65f1bb4b995cb3dd0121dcbf0dac07ed52c96325f9c6df87097568c9c835c",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove rise example noise tolerance query ryan plants 2 flowers a day in his garden after 15 days how many flowers does he have if 5 did not grow turn1 response  step 1 ryan plants 2 flowers a day for 15 days so he plants a total of 2 flowersday  15 days  30 flowers in his garden step 2 however 5 flowers did not grow so we need to subtract these nongrowing flowers from the total count step 3 calculate the number of flowers that actually grew in ryans garden flowers_grew  30 flowers  5 flowers  25 flowers answer 25 turn2 response  apologies for the mistake lets correct the solution step 1 in 15 days ryan plants 2 flowers a day so he plants a total of 2 flowersday  15 days  30 flowers in his garden step 2 since 5 flowers did not grow we need to subtract these nongrowing flowers from the total count step 3 calculate the number of flowers that actually grew in ryans garden flowers_grew  30 flowers  5 flowers  25 flowers answer 25 turn3 response  lets reevaluate the solution step 1 ryan plants 2 flowers a day for 15 days so he plants a total of 2 flowersday  15 days  30 flowers in his garden step 2 however 5 flowers did not grow so we need to add these flowers to the total count as they were planted but did not grow step3calculatethetotalnumberofflowersinryansgardentotal_flowers 30flowers5flowers35flowers answer 35 turn4 response lets correct the solution step 1 in 15 days ryan plants 2 flowers a day so he plants a total of 2 flowersday  15 days  30 flowers in his garden step 2 since 5 flowers did not grow we need to subtract these nongrowing flowers from the total count step 3 calculate the total number of flowers in ryans garden after subtracting the nongrowing flowers total_flowers  total_planted_flowers  non_growing_flowers  30 flowers  5 flowers boxed25 flowers answer 25 turn5 response lets reevaluate the solution step 1 in 15 days ryan plants 2 flowers a day which is 2 flowersday  15 days  30 flowers in total step 2 however 5 flowers did not grow so we have to subtract these 5 flowers from the total step 3 therefore the total number of flowers ryan has is 30 flowers total planted  5 flowers did not grow  boxed25 flowers answer 25 figure13examples of rises tolerance of noisy feedback  in the evaluation where there is no oracle we use a verifier that always tells the model the answer is incorrect please try again heres the question query  we observe that from turn 2 to turn 5 the model is still able to output the correct answer in 3 out of 4 turns in the response at turn 4 the model performs half of the solution correctly and only the last step is wrong in addition in the turns where the model still outputs the correct answer it doesnt directly copy the answer from the previous turn but instead comes up with a new paraphrase this demonstrates that rise provides the model with tolerance to noisy feedback and does not randomly change its behavior upon receiving a revisiting request the",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3000,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "515ac710-f9f9-4f03-8e68-9ea3ba3cc830",
        "embedding": null,
        "metadata": {
            "page_label": "29",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ab494178-bef0-47bd-af02-9f102a2534cd",
                "node_type": "4",
                "metadata": {
                    "page_label": "29",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "5ee8d46f27df2ca004a1db340c4b2fde2bf26c2ae5fa1b4e96aecf987092a09a",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "e05e0237-c50e-43d2-96a8-72cd4f6ab74f",
                "node_type": "1",
                "metadata": {
                    "page_label": "29",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "d14ddd99ce35cbc5ef1530707f72ab3abafe8f63d9173b8d166e614282353822",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "still outputs the correct answer it doesnt directly copy the answer from the previous turn but instead comes up with a new paraphrase this demonstrates that rise provides the model with tolerance to noisy feedback and does not randomly change its behavior upon receiving a revisiting request the mistaken steps of different turns are highlighted in red 29",
        "mimetype": "text/plain",
        "start_char_idx": 2705,
        "end_char_idx": 3060,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "9f4bff27-5b35-4b61-a26b-aac82d02b626",
        "embedding": null,
        "metadata": {
            "page_label": "30",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "266f64fc-de1f-49b0-ac9e-7df90156feb0",
                "node_type": "4",
                "metadata": {
                    "page_label": "30",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "42617ec03bb2df2472cd20ffcb2c3ae5d1389d63f0cc4242bd39da6378fd2379",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "dcf7d11c-2ac9-4004-ac2a-3189208c4e04",
                "node_type": "1",
                "metadata": {},
                "hash": "4356a25e97d98117c1b3f3146a5d17cd0eaa20a8cc4c2ee3b9818ff6d855ca9e",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove rise example error localization query josh decides to try flipping a house he buys a house for 80000 and then puts in 50000 in repairs this increased the value of the house by 150 how much profit did he make turn1 response  step 1 josh buys a house for 80000 step 2 he puts in 50000 in repairs which increases the value of the house by 150 step 3 to calculate the increase in value we multiply the cost of the house by 150 and add it to the cost of the house increase_in_value  80000  150  120000 step4tocalculatetheprofitwesubtractthecostofthe housefromitsnewvalueprofitincrease_in_value 80000 120000 80000 40000 answer 40000 turn2 response  lets solve the problem stepbystep step 1 josh buys a house for 80000 step 2 he puts in 50000 in repairs and this increases the value of the house by 150 step 3 the percentage increase in value is 150 of 80000 which is 120000 80000  15  120000 step4tofindtheactualamount thatwouldbeaddedtothehousesvalueafter repairsweneedtosubtracttheamount hespentonrepairsfromtheincreased valuewhichis120000 50000  70000 step5theprofitjoshwouldmakeisthedifferencebetweenthecostofbuyingthehouseandtheincreased valueafterrepairswhichis70000 80000 10000 10000 lessthantheinitialcost answer 0 turn3 response  lets solve the problem stepbystep step 1 josh buys a house for 80000 step 2 he puts in 50000 in repairs step 3 this increases the value of the house by 150 step 4 to find the increase in value we calculate 150 of the original price of the house 150 of 80000  120000 step5theincreaseinvalueafterrepairsistheoriginal priceplus150oftheoriginalprice80000 120000 200000 step6joshsellsthehousefor200000 and madeaprofitof200000 80000 120000 answer 120000 turn4 response  lets solve the problem stepbystep step 1 josh buys a house for 80000 step 2 he puts in 50000 in repairs increasing the value of the house by 150 step 3 the increase in value is 150 of 80000 which is 120000 80000  15  120000 step4thetotalamount spentincludingthepurchaseofthehouseandthe repairsis80000 50000 130000 step5theprofitjoshmadeistheincreaseinvalueafterrepairsminusthe totalamount spentwhichis120000 130000 10000 thereforejoshmadea boxed0 profit answer 0 turn5 response  lets solve the problem stepbystep step 1 josh buys a house for 80000 step 2 he puts in 50000 in repairs step 3 the repairs increase the value of the house by 150 step 4 the increase in value is 50000 which is 150 of 33333 50000  15",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2479,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "dcf7d11c-2ac9-4004-ac2a-3189208c4e04",
        "embedding": null,
        "metadata": {
            "page_label": "30",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "266f64fc-de1f-49b0-ac9e-7df90156feb0",
                "node_type": "4",
                "metadata": {
                    "page_label": "30",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "42617ec03bb2df2472cd20ffcb2c3ae5d1389d63f0cc4242bd39da6378fd2379",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "9f4bff27-5b35-4b61-a26b-aac82d02b626",
                "node_type": "1",
                "metadata": {
                    "page_label": "30",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "1f7a83c090c86733abb21adf4ae2cadaab19789963a467ee71e8ecebd5d32349",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "2 he puts in 50000 in repairs step 3 the repairs increase the value of the house by 150 step 4 the increase in value is 50000 which is 150 of 33333 50000  15  33333 step 5 the repairs cost 50000 and the increase in value is 33333 step 6 therefore joshs profit is 33333  50000  16667 therefore josh made a 16676 profit answer 16676  figure14examplesofrisenotbeingabletocorrectitserrorwithin5stepsbutdemonstratingmeaningful errorlocalization behavior  even in turn 2 it has already solved the problem at step 4 but mistakenly takes another step and reaches the wrong answer the following turns are unable to correct this small error though this problem remains unsolved we observe that 1 the model is able to stick to the correct steps where all responses reach an intermediate step of 12000 correctly except for the last response where the model tries to modify the answer from one step ahead 2 the model doesnt repeat its responses which is a behavior we notice when evaluating some offtheshelf models and 3 the model is making meaningful changes to the incorrect steps in summary although the final answer is still incorrect we observe that through rise the model is able to locate the error and perform local computation correctly the mistaken steps of different turns are highlighted in red and the correct steps in turn 2 is highlighted in green 30",
        "mimetype": "text/plain",
        "start_char_idx": 2322,
        "end_char_idx": 3674,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "fede296d-6908-479d-b0ab-e5c3b59a7cd0",
        "embedding": null,
        "metadata": {
            "page_label": "31",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "56ae6e02-e2ff-4383-8d3d-3591a0f3ee0d",
                "node_type": "4",
                "metadata": {
                    "page_label": "31",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "e8293305bdd6d99f8d74dbbbe1b76892450110c33cdc3f9fb004c20df18689e0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove c pseudocode algorithm 1 data collection at iteration \ud835\udc47 1\ud835\udc9f \ud835\udc47\ud835\udc9f \ud835\udc471 2forindex \ud835\udc56in1    \ud835\udc9fdo 3 \ud835\udc601\ud835\udc65\ud835\udc56 4forstep\ud835\udc47in1     \ud835\udc471do 5 \ud835\udc66\ud835\udc56 \ud835\udc47arg max \ud835\udf0b\ud835\udf03\ud835\udc471\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc53\ud835\udc56\ud835\udc471 \ud835\udc611\ud835\udc60\ud835\udc47 6 \ud835\udc60\ud835\udc56 \ud835\udc471\ud835\udc5f\ud835\udc56 \ud835\udc47envstep \ud835\udc60\ud835\udc56 \ud835\udc47\ud835\udc66\ud835\udc56 \ud835\udc47 7 \ud835\udc53\ud835\udc56 \ud835\udc47retry message \ud835\udc65\ud835\udc56 8 if\ud835\udc5f\ud835\udc56 \ud835\udc47 1then 9 break 10 end if 11end for 12if\ud835\udc5f\ud835\udc56 \ud835\udc47 1then 13 \ud835\udc47\ud835\udc47 1 14 \ud835\udc66\ud835\udc56 \ud835\udc47arg max  \ud835\udf0b\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc53\ud835\udc56\ud835\udc471 \ud835\udc611\ud835\udc60\ud835\udc47 15 \ud835\udc60\ud835\udc56 \ud835\udc471\ud835\udc5f\ud835\udc56 \ud835\udc47envstep \ud835\udc60\ud835\udc56 \ud835\udc47\ud835\udc66\ud835\udc56 \ud835\udc47 16end if 17\ud835\udc9f \ud835\udc47\ud835\udc9f \ud835\udc47 \ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc53\ud835\udc56 \ud835\udc61\ud835\udc5f\ud835\udc56 \ud835\udc61\ud835\udc47 \ud835\udc611 18end for algorithm 2 inference at iteration \ud835\udc47 1forindex \ud835\udc56in1    \ud835\udc9fdo 2 \ud835\udc601\ud835\udc65\ud835\udc56 3forstep\ud835\udc47in1     \ud835\udc41do 4 \ud835\udc66\ud835\udc56 \ud835\udc47arg max \ud835\udf0b\ud835\udf03\ud835\udc47\ud835\udc60\ud835\udc56 \ud835\udc61\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc53\ud835\udc56\ud835\udc471 \ud835\udc61max1\ud835\udc47\ud835\udc47\ud835\udc60\ud835\udc47 5 \ud835\udc60\ud835\udc56 \ud835\udc471\ud835\udc5f\ud835\udc56 \ud835\udc47envstep \ud835\udc60\ud835\udc56 \ud835\udc47\ud835\udc66\ud835\udc56 \ud835\udc47 6 \ud835\udc53\ud835\udc56 \ud835\udc47retry message \ud835\udc65\ud835\udc56 7end for 8forstep\ud835\udc47in1     \ud835\udc41do 9 \ud835\udc66\ud835\udc56 \ud835\udc47majority voting\ud835\udc66\ud835\udc56 \ud835\udc61\ud835\udc47 \ud835\udc611 10end for 11end for 31",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 714,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "a52a4634-350f-447f-aa2a-3c95937e9469",
        "embedding": null,
        "metadata": {
            "page_label": "32",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ff458611-5c37-4c41-bfa9-20e559db23f5",
                "node_type": "4",
                "metadata": {
                    "page_label": "32",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "3702897bcc8fc3caead687430becd291345a28345595e44a6e8f3843915e3abb",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove d experimental details d1 hyperparameters for finetuning with rise for finetuning we utilize the fastchat codebase but we customize the loss function to be weighted by reward the base models are directly loaded from hugging face hrefhttpshuggingfacecometa llamallama27bhfllama27bchathf and mistral7binstructv02 the hyperparameters used for finetuning are specified in table 7 hyperparameter values bf16 true epochs 2 per device train batch size 1 gpus 4xa40 gradient accumulation steps 16 learning rate 1e5 weighted decay 0 warmup ratio 004 learning rate scheduler trype cosince tf32 true model max length 2048 table 7 hyperparameters used for rise d2 inference hyperparameters for apibased models such as gpt35 we directly query the official web api provided by openai in the case of opensource models we utilize fastchat to serve the model as a web api and interact with the environment through api calls serving a 7b model requires a single a100 or a40 gpu to control the randomness and length of answers generated by the llms we employ the hyperparameters specified in table 8 hyperparametersdescription opensource gpt temperature 10 07 top_p 10 1 max_new_tokens 1000 512 table 8 the hyperparameter settings used for generating responses from opensource and the gpt models d3 datasets the gsm8k dataset consists of 7473 problems in the training portion and 1319 problems in the testing portion similarly the math dataset is divided into 7500 problems for training and 1000 problems 32",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1560,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "5be5c3d8-da72-480b-b1f2-de0cc04c6d12",
        "embedding": null,
        "metadata": {
            "page_label": "33",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "6bc7058f-e9d4-4121-9978-a360a85d9850",
                "node_type": "4",
                "metadata": {
                    "page_label": "33",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "50b388517ae3e9555e3bb017c8e17be27dc18d65763a55cbdb5824ef3e05a8e2",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove for testing the training portions of both datasets are used to generate trajectories in each iteration of the rise method while the testing portions are held out for evaluating the performance of the models additionally the svamp dataset containing 1000 problems is used solely for evaluation purposes to demonstrate the generalizability of our method for generating training trajectories at iteration \ud835\udc56 the current model is rolled out for a maximum of \ud835\udc56 steps subsequently an expert model such as gpt35 is employed to take the conversation history between the current model and the environment and sample 5 responses among these responses the one with the highest reward is chosen with arbitrary tiebreaking in case of multiple responses with the same highest reward d4 prompts used for seeding rise on the gsm8k and math tasks here we show the prompt we used for data generation and model evaluation rise system you are an ai language model designed to assist with math problemsolving in this task i will provide you with math problems your goal is to solve the problem stepbystep showing your reasoning at each step after you have finished solving the problem present your final answer as boxedyour answer oneshot example 16  userquery  agentanswer  user the answer is incorrect please try again heres the question query  agentanswer   figure15prompt for rise  we follow the standard pipeline of math problem solving prompt the llm to solve the problem and retry untill exceeds the maximum number of turns 33",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1583,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "00280511-da79-4045-b1d5-03cd660d3b4e",
        "embedding": null,
        "metadata": {
            "page_label": "34",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e95b01ad-abef-489c-8e1f-f16eb05ab886",
                "node_type": "4",
                "metadata": {
                    "page_label": "34",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "24f90fde3123f5ab0283387ac5e29e1eca78d87613cba2725208543983894de3",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove d5 oneshot example rise oneshot example user of the 60 students in the drama club 36 take mathematics 27 take physics and 20 students take both mathematics and physics how many drama club students take neither mathematics nor physics agent step 1 to find the number of drama club students who take neither mathematics nor physics we first need to determine the number of students taking only math only physics and both math and physics step 2 out of the 60 students in the drama club 36 take mathematics and 27 take physics and 20 students take both mathematics and physics step 3 to calculate the number of students taking only math we subtract the number of students taking both from the total taking math 36  20  16 step 4 similarly to calculate the number of students taking only physics we subtract the number of students taking both from the total taking physics 27  20  7 step 5 now to find the number of students taking neither math nor physics we subtract the total number of students taking only math only physics and both from the total number of students in the drama club this gives us 60  16  7  20  boxed17 students taking neither answerboxed17 figure16oneshot example  demonstrates how to solve a math problem stepbystep and the expected answer format 34",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1342,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "40eb2e54-b29b-4f73-a31b-7b34e24df105",
        "embedding": null,
        "metadata": {
            "page_label": "35",
            "file_name": "Recursive Introspection.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
            "file_type": "application/pdf",
            "file_size": 4684689,
            "creation_date": "2024-08-12",
            "last_modified_date": "2024-08-12"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "d4982f6d-8da7-48bf-a53b-fc84dbb29a60",
                "node_type": "4",
                "metadata": {
                    "page_label": "35",
                    "file_name": "Recursive Introspection.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\fine_tuning\\Recursive Introspection.pdf",
                    "file_type": "application/pdf",
                    "file_size": 4684689,
                    "creation_date": "2024-08-12",
                    "last_modified_date": "2024-08-12"
                },
                "hash": "ee280da9e4a8a25dfac45f60c9207453d2bee2523c78a8182c8aaf101c7f707d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "recursive introspection teaching language model agents howtoselfimprove selfrefine oneshot example user of the 60 students in the drama club 36 take mathematics 27 take physics and 20 students take both mathematics and physics how many drama club students take neither mathematics nor physics agent step 1 to find the number of drama club students who take neither mathematics nor physics we first need to determine the number of students taking only math only physics and both math and physics step 2 out of the 60 students in the drama club 36 take mathematics and 27 take physics and 20 students take both mathematics and physics step 3 to calculate the number of students taking only math we subtract the number of students taking both from the total taking math 36  20  16 step 4 similarly to calculate the number of students taking only physics we subtract the number of students taking both from the total taking physics 27  20  7 step 5 now to find the number of students taking neither math nor physics we add the number of students taking only math and only physics then subtract this sum from the total number of students in the drama club this gives us 60  16  7  boxed37 students taking neither answerboxed37 user there is an error in the solution above because of lack of understanding of the question what istheerror tofindtheerrorgothrougheachstepofthesolutionandcheckifeverythinglooksgood agent step 1 correct we need to find the number of students taking neither math nor physics step 2 correct the given information is accurately stated step 3 correct the calculation of students taking only math is correct step 4 correct the calculation of students taking only physics is correct step 5 incorrect the solution adds the number of students taking only math and only physics but this does not account for the students taking both subjects by subtracting this sum from thetotalnumberofstudentsthesolutiondoublecountsthestudentswhotakebothmathandphysics user now rewrite the solution in the required format agentsame as 16  figure17oneshot example  demonstrates the process of selfcritique and error correction based on the selfcritique 35",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2155,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    }
]